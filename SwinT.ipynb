{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07622a2e",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3febf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xflow-py\n",
    "from xflow import ConfigManager, FileProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.utils import load_validated_config, plot_image\n",
    "\n",
    "from config_utils import load_config\n",
    "\n",
    "# Configuration\n",
    "config_manager = ConfigManager(load_config(\"SwinT.yaml\"))\n",
    "config = config_manager.get()\n",
    "config_manager.add_files(config[\"extra_files\"])\n",
    "\n",
    "# ==================== \n",
    "# Prepare Dataset\n",
    "# ====================\n",
    "provider = FileProvider(config[\"paths\"][\"datasets\"][\"syns\"]).\\\n",
    "    subsample(fraction=config[\"data\"][\"subsample_fraction\"], seed=config[\"seed\"]) #\n",
    "train_provider, temp_provider = provider.split(ratio=config[\"data\"][\"train_val_split\"], seed=config[\"seed\"])\n",
    "val_provider, test_provider = temp_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "\n",
    "def make_dataset(provider):\n",
    "    return PyTorchPipeline(provider, transforms).to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "\n",
    "train_dataset = make_dataset(train_provider)\n",
    "val_dataset = make_dataset(val_provider)\n",
    "test_dataset = make_dataset(test_provider)\n",
    "\n",
    "print(\"Samples: \",len(train_provider),len(val_provider),len(test_provider))\n",
    "print(\"Batch: \",len(train_dataset),len(val_dataset),len(test_dataset))\n",
    "\n",
    "for left_parts, right_parts in test_dataset:\n",
    "    # batch will be a tuple: (right_halves, left_halves) due to split_width\n",
    "    print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "    plot_image(left_parts[0])\n",
    "    plot_image(right_parts[0])\n",
    "    break\n",
    "\n",
    "# ==================== \n",
    "# Construct Model\n",
    "# ====================\n",
    "import torch\n",
    "from SwinT import SwinUNet\n",
    "\n",
    "model = SwinUNet(\n",
    "    in_chans=config[\"model\"][\"in_chans\"],\n",
    "    out_chans=config[\"model\"][\"out_chans\"],\n",
    "    )  # 224x224x1 -> 224x224x1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "show_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e3d9a-a14b-4761-9e01-a07c4060a084",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2936ffe-6ba6-4cd9-a050-4d15306da638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_beam_param_metric\n",
    "from functools import partial\n",
    "\n",
    "import xflow.extensions.physics\n",
    "from xflow.trainers import TorchTrainer, build_callbacks_from_config\n",
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "# 1) device/model/optim/loss\n",
    "\n",
    "# Option A: generic image intensity reconstruction\n",
    "criterion = torch.nn.L1Loss()  # or Charbonnier.  torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=config['training']['learning_rate'], \n",
    "                             weight_decay=config['training']['weight_decay'])\n",
    "\n",
    "# 2) callbacks (unchanged) + any custom wiring\n",
    "callbacks = build_callbacks_from_config(\n",
    "    config=config[\"callbacks\"],\n",
    "    framework=config[\"framework\"],  \n",
    ")\n",
    "callbacks[-1].set_dataset(test_dataset)  # keep dataset closure\n",
    "\n",
    "# Extract beam parameters\n",
    "extract_beam_parameters_dict = partial(extract_beam_parameters, as_array=False)\n",
    "beam_param_metric = make_beam_param_metric(extract_beam_parameters_dict)\n",
    "\n",
    "# 3) run training\n",
    "trainer = TorchTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    output_dir=config[\"paths\"][\"output\"],\n",
    "    \n",
    "    data_pipeline=train_dataset,\n",
    "    val_metrics=[beam_param_metric]\n",
    ")\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=train_dataset, \n",
    "    val_loader=val_dataset,\n",
    "    epochs=config['training']['epochs'],\n",
    ")\n",
    "\n",
    "# 4) persist\n",
    "trainer.save_history(f\"{config['paths']['output']}/history.json\")\n",
    "trainer.save_model(config[\"paths\"][\"output\"])  # uses model.save_model(...) if available\n",
    "config_manager.save(output_dir=config[\"paths\"][\"output\"], config_filename=config[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f01f4-2b4f-4fec-a675-58945757ce5f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbd54e-8e12-4896-a5e8-37dd302c4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
