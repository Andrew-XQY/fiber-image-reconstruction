{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07622a2e",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3febf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xflow-py\n",
    "from xflow import ConfigManager, FileProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.trainers import build_callbacks_from_config, build_callbacks_from_config\n",
    "from xflow.utils import load_validated_config, plot_image\n",
    "\n",
    "from TM import TransmissionMatrix\n",
    "\n",
    "# Configuration\n",
    "config_manager = ConfigManager(load_validated_config(\"SwinT.yaml\"))\n",
    "config = config_manager.get()\n",
    "config_manager.add_files(config[\"extra_files\"])\n",
    "\n",
    "# Data pipeline\n",
    "provider = FileProvider(config[\"paths\"][\"dataset\"]).subsample(fraction=1)\n",
    "train_provider, temp_provider = provider.split(ratio=config[\"data\"][\"train_val_split\"], \n",
    "                                               seed=config[\"seed\"])\n",
    "val_provider, test_provider = temp_provider.split(ratio=config[\"data\"][\"val_test_split\"], \n",
    "                                                  seed=config[\"seed\"])\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "\n",
    "def make_dataset(provider):\n",
    "    return PyTorchPipeline(provider, transforms).to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "\n",
    "train_dataset = make_dataset(train_provider)\n",
    "val_dataset = make_dataset(val_provider)\n",
    "test_dataset = make_dataset(test_provider)\n",
    "\n",
    "print(\"Samples: \",len(train_provider),len(val_provider),len(test_provider))\n",
    "print(\"Batch: \",len(train_dataset),len(val_dataset),len(test_dataset))\n",
    "\n",
    "for left_parts, right_parts in test_dataset:\n",
    "    # batch will be a tuple: (right_halves, left_halves) due to split_width\n",
    "    print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "    plot_image(left_parts[0])\n",
    "    plot_image(right_parts[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3330de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Model building\n",
    "# ====================================\n",
    "import torch\n",
    "from SwinT import SwinV2AutoEncoder\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Explicit model construction\n",
    "# ----------------------------\n",
    "model_config = config[\"model\"]\n",
    "model = SwinV2AutoEncoder(\n",
    "    img_size=model_config[\"img_size\"],\n",
    "    in_chans=model_config[\"in_chans\"], \n",
    "    out_chans=model_config[\"out_chans\"],\n",
    "    patch_size=model_config[\"patch_size\"],\n",
    "    embed_dim=model_config[\"embed_dim\"],\n",
    "    depths=model_config[\"depths\"],\n",
    "    num_heads=model_config[\"num_heads\"],\n",
    "    window_size=model_config[\"window_size\"],\n",
    "    mlp_ratio=model_config[\"mlp_ratio\"],\n",
    "    qkv_bias=model_config[\"qkv_bias\"],\n",
    "    drop_path=model_config[\"drop_path\"],\n",
    "    decoder_channels=model_config[\"decoder_channels\"],\n",
    "    final_activation=model_config[\"final_activation\"]\n",
    ")\n",
    "\n",
    "show_model_info(model)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Training loop prepare\n",
    "# ====================================\n",
    "import xflow.extensions.physics\n",
    "\n",
    "# criterion = torch.nn.MSELoss()  # pixel level MSE\n",
    "criterion = torch.nn.L1Loss()  # start with L1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "\n",
    "callbacks = build_callbacks_from_config(\n",
    "    config=config[\"callbacks\"],\n",
    "    framework=config[\"framework\"],\n",
    ")\n",
    "callbacks[-1].set_dataset(test_dataset)  # add dataset closure to the last callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e3d9a-a14b-4761-9e01-a07c4060a084",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7b5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_feature_map(features, epoch, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.imsave(\n",
    "        os.path.join(output_dir, f'epoch_{epoch}_featuremap.png'),\n",
    "        features[0, 0].detach().cpu().numpy(),\n",
    "        cmap='viridis'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2936ffe-6ba6-4cd9-a050-4d15306da638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "for cb in callbacks:\n",
    "    cb.on_train_begin(epochs=config['training']['epochs'])  # Pass total epochs\n",
    "\n",
    "for epoch in range(config['training']['epochs']):\n",
    "    for cb in callbacks:\n",
    "        cb.on_epoch_begin(epoch, model=model, total_batches=len(train_dataset))\n",
    "\n",
    "    model.train()\n",
    "    train_loss_sum, n_train = 0.0, 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataset):\n",
    "        for cb in callbacks: \n",
    "            cb.on_batch_begin(batch_idx)\n",
    "\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # ----------------- debugging -----------------\n",
    "        features = model.get_bottleneck_features(inputs)  # shape: [B, 768, 8, 8]\n",
    "        save_feature_map(features, epoch, config[\"paths\"][\"output\"])\n",
    "        # Inspect decoder weights statistics\n",
    "        model.print_decoder_weights_stats()\n",
    "        # --------------------------------------------\n",
    "\n",
    "        optimizer.zero_grad()                      # 1. clear gradients\n",
    "        outputs = model(inputs)                    # 2. forward pass\n",
    "        loss = criterion(outputs, targets)         # 3. compute loss\n",
    "        loss.backward()                            # 4. backprop\n",
    "        optimizer.step()                           # 5. update weights\n",
    "\n",
    "        # Convert tensor to float for logging\n",
    "        for cb in callbacks:\n",
    "            cb.on_batch_end(batch_idx, logs={\"loss\": loss.item()})\n",
    "\n",
    "        # accumulate for epoch average\n",
    "        train_loss_sum += loss.item()\n",
    "        n_train += 1\n",
    "\n",
    "    avg_train_loss = train_loss_sum / max(1, n_train)\n",
    "\n",
    "    # ----- validation pass (no grad) -----\n",
    "    model.eval()\n",
    "    val_loss_sum, n_val = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_dataset:       # assume val_dataset is ready\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss_sum += criterion(val_outputs, val_targets).item()\n",
    "            n_val += 1\n",
    "    avg_val_loss = val_loss_sum / max(1, n_val)\n",
    "\n",
    "    # Add epoch-level metrics if available\n",
    "    for cb in callbacks:\n",
    "        cb.on_epoch_end(epoch, logs={\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        })\n",
    "\n",
    "# Call once after training\n",
    "for cb in callbacks:\n",
    "    cb.on_train_end()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save final trained model for inference\n",
    "import os\n",
    "os.makedirs(config[\"paths\"][\"output\"], exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(config[\"paths\"][\"output\"], 'swin_model.pth'))\n",
    "\n",
    "config_manager.save(output_dir=config[\"paths\"][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f01f4-2b4f-4fec-a675-58945757ce5f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbd54e-8e12-4896-a5e8-37dd302c4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SwinT import build_model\n",
    "import torch\n",
    "\n",
    "# Rebuild model and load weights\n",
    "model = build_model(config['model'])\n",
    "model.load_state_dict(torch.load('results/swin_model.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test reconstruction\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataset:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Visualize results\n",
    "        print(f\"Test MSE Loss: {torch.nn.MSELoss()(outputs, targets).item():.6f}\")\n",
    "        \n",
    "        # Plot some examples\n",
    "        from xflow.utils import plot_image\n",
    "        plot_image(inputs[:4].cpu())  # Input images\n",
    "        plot_image(outputs[:4].cpu())  # Reconstructed images\n",
    "        plot_image(targets[:4].cpu())  # Target images\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
