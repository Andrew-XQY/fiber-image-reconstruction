# ====================
# Conv AutoEncoder fiber image reconstruction configuration file
# ====================

seed: &seed 42                                  # 42, 168, 500
name: &config "CAE_validate_clear.yaml"
framework: "pytorch"
file_extract: False

paths:
  output: ${paths.output_root}                  # Comes from shared defaults
  clear_yag: ${paths.datasets.clear_yag}          # Picked by machine profile
  clear_laser: ${paths.datasets.clear_laser} 
  clear_chromox_wednesday: ${paths.datasets.clear_chromox_wednesday}
  clear_chromox_friday: ${paths.datasets.clear_chromox_friday}
  clear_chromox_saturday: ${paths.datasets.clear_chromox_saturday}
  processed_pure_dmd: ${paths.datasets.processed_pure_dmd}

model:
  name: "CAE"                             # Factory key for model architecture
  in_channels: 1
  out_channels: 1
  kernel_size: 4
  encoder:          [64,  128, 128, 256, 512, 512]
  decoder:          [512, 512, 256, 128, 128, 64 ]
  apply_batchnorm:  [0,   1,   1,   1,   1,   1  ]
  apply_dropout:    [1,   1,   0,   0,   0,   0  ]
  final_activation: sigmoid

training:
  epochs: &epochs 100                           # Number of full passes over training data
  learning_rate: 1.0e-4                         # Initial learning rate for optimizer
  batch_size: &batch 32                         # Number of samples per gradient update

callbacks:
  - name: "torch_batch_progress_bar"
    params:
      only_keys: ["train_loss", "val_loss"]
  - name: "torch_early_stopping"                # Factory key for early stopping
    params:
      monitor: "val_loss"                       # Metric to monitor
      patience: 20                              # Epochs to wait before stopping
  - name: "torch_image_reconstruction_callback"
    params:
      save_dir: ${paths.output}                 # Directory to save results 

simulation:
  canvas_size: [256, 256]                                                  
  total_Guassian_num: 100  
  minimum_pixel_threshold: 10.0
  max_intensity: 70
  fade_rate: 0.98
  std_1: 0.02
  std_2: 0.08
  distribution: "other" 
  process_functions:
    - name: "remap_range"               # Normalize pixel values to [0,1]
    - name: "resize"                   # Resize to match input dimensions
      params:
        size: [32, 32]
        interpolation: "bilinear"


data:
  total_train_samples: 1000                     
  train_val_split: 0.8                          # Train/validation split ratio 80/20
  val_test_split: 0.5                           # Validation/test split ratio 50/50
  crop_gt: &crop_gt [[742, 341], [1202, 801]]
  crop_fo: &crop_fo [[360, 0], [1560, 1200]]
  input_shape: &inp [256, 256]                  # Speckle image dimensions [H, W]
  output_shape: &out [256, 256]                 # Input pattern size [H
  transforms:
    torch:
      - name: "torch_load_image"                # Load the image based on file path
      - name: "torch_to_tensor"                 # Convert to tensor
      # - name: "torch_to_grayscale"              # Convert to grayscale
      - name: "torch_remap_range"               # Normalize pixel values to [0,1]
      - name: "torch_split_width"               # Split image into left/right halves on width
        params:
          swap: True       
      - name: "multi_transform"                 # normalized the orthogonal basis (but exclude on the val/test set)
        params:
          transforms:
            - name: "identity"                  # Fiber output   
            # - name: "torch_threshold"          
            #   params:
            #     threshold: 5.0
            #     threshold_value: 0.0
            #     threshold_mode: "below"       
            - name: "torch_threshold"           # Ground truth, but should we clip the background noise? since this change the source of truth mapping relation?
              params:
                threshold: 0.098
                threshold_value: 0.0
                threshold_mode: "below"        

  post_transforms:
    torch:
      # - name: "multi_transform"                 # normalized the orthogonal basis (but exclude on the val/test set)
      #   params:
      #     transforms:
      #       - name: "torch_crop_area"        
      #         params:
      #           points: *crop_fo
      #       - name: "torch_crop_area"          # Ground truth
      #         params:
      #           points: *crop_gt
      # - name: "multi_transform"                 # Resize each cropped region
      #   params:
      #     transforms:
      #       - name: "torch_resize"    
      #         params:
      #           size: *inp
      #           interpolation: "bilinear"
      #       - name: "torch_resize"
      #         params:
      #           size: *out
      #           interpolation: "bilinear"
      - name: "multi_transform"                
        params:
          transforms:
            - name: "torch_clip_range"   
              params:
                clip_min: 0
                clip_max: 1
            - name: "torch_clip_range"        
              params:
                clip_min: 0
                clip_max: 1


  dataset_ops:                                  # Dataset level operations
    - name: "torch_batch"                       # Batch images
      params:
        batch_size: *batch                      # Number of images per batch
        shuffle: True                           # Shuffle dataset
        num_workers: 0                          # Number of worker threads for data loading
        prefetch_factor: 2                      # Number of batches to prefetch
        seed: *seed                             # Random seed for reproducibility

extra_files: ["models/CAE.py"]