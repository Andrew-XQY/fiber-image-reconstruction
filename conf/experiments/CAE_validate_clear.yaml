# ====================
# Conv AutoEncoder fiber image reconstruction configuration file
# ====================

seed: &seed 42                                  # 42, 168, 500
name: &config "CAE.yaml"
framework: "pytorch"
file_extract: False

paths:
  output: ${paths.output_root}                  # Comes from shared defaults
  dataset: ${paths.datasets.clear_yag}          # Picked by machine profile
  training_set: ${paths.datasets.clear_laser} 
  test_set: ${paths.datasets.clear_yag} 
  chromox_2025-11-19: ${paths.datasets.clear_chromox_wednesday}
  chromox_2025-11-21: ${paths.datasets.clear_chromox_friday}
  chromox_2025-11-22-morning: ${paths.datasets.clear_chromox_saturday}

model:
  in_channels: 1
  out_channels: 1
  kernel_size: 4
  encoder:          [64,  128, 128, 256, 512, 512]
  decoder:          [512, 512, 256, 128, 128, 64 ]
  apply_batchnorm:  [0,   1,   1,   1,   1,   1  ]
  apply_dropout:    [1,   1,   0,   0,   0,   0  ]
  final_activation: sigmoid

training:
  epochs: &epochs 100                           # Number of full passes over training data
  learning_rate: 1.0e-4                         # Initial learning rate for optimizer
  batch_size: &batch 32                         # Number of samples per gradient update

callbacks:
  - name: "torch_batch_progress_bar"
    params:
      only_keys: ["train_loss", "val_loss"]
  - name: "torch_early_stopping"                # Factory key for early stopping
    params:
      monitor: "val_loss"                       # Metric to monitor
      patience: 20                              # Epochs to wait before stopping
  - name: "torch_image_reconstruction_callback"
    params:
      save_dir: ${paths.output}                 # Directory to save results 

data:
  train_val_split: 0.8                          # Train/validation split ratio 80/20
  val_test_split: 0.5                           # Validation/test split ratio 50/50
  input_shape: &inp [256, 256]                  # Speckle image dimensions [H, W]
  output_shape: &out [256, 256]                 # Input pattern size [H, W]
  crop_groud_truth: &crop_gt [[83, 178], [160, 101]] # [[871, 554], [998, 680]]   [[78, 181], [163, 98]]   [[863, 661], [997, 577]]
  crop_fiber_output: &crop_fo [[0, 0], [255, 255]] # [[360, 0], [1560, 1200]]   [[0, 0], [255, 255]]
  transforms:
    torch:
      - name: "torch_load_image"                # Load the image based on file path
      - name: "torch_to_tensor"                 # Convert to tensor
      - name: "torch_to_grayscale"              # Convert to grayscale
      - name: "torch_remap_range"               # Normalize pixel values to [0,1]
      - name: "torch_split_width"               # Split image into left/right halves on width
        params:
          swap: True     
      - name: "multi_transform"
        params:
          transforms:
            - name: "identity"                  # Change back channel order after cropping
            - name: "check_centroid"            # Change channel order for cropping
              params:
                rect: *crop_gt                  # From [C, H, W] to [H, W, C]
                method: "first_moment"          # Centroid calculation method, beam_params; first_moment
                on_fail: "raise"                # first is fiber output, second is ground truth !! (for training logic convenience)
      - name: "multi_transform"                 # Crop different regions for each half
        params:
          transforms:
            - name: "torch_crop_area"
              params:
                points: *crop_fo
            - name: "torch_crop_area"
              params:
                points: *crop_gt
      - name: "multi_transform"                 # Resize each cropped region
        params:
          transforms:
            - name: "torch_resize"        
              params:
                size: *inp                      # Resize fiber image to dimensions [H, W]
            - name: "torch_resize"        
              params:
                size: *out                      # Resize ground truth image to dimensions [H, W]
  dataset_ops:                                  # Dataset level operations
    - name: "torch_batch"                       # Batch images
      params:
        batch_size: *batch                      # Number of images per batch
        shuffle: True                           # Shuffle dataset
        num_workers: 0                          # Number of worker threads for data loading
        prefetch_factor: 2                      # Number of batches to prefetch
        seed: *seed                             # Random seed for reproducibility

extra_files: ["models/CAE.py"]