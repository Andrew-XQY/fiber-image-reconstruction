{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c2bb89",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Load trained model, predict on the test data, save all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60e48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Only run once'''\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.chdir(Path.cwd().parent) # set the correct working directory\n",
    "\n",
    "from evaluation.evaluation import *\n",
    "from __future__ import annotations\n",
    "from xflow import ConfigManager, FileProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.utils import load_validated_config, save_image, plot_image\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime  \n",
    "from config_utils import load_config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "\n",
    "def fixed_process(experiments, experiment, count, batch_size=None, change_val_dir=None):\n",
    "  # Load the trained model\n",
    "  from models.CAE import Autoencoder2D\n",
    "  from models.ERN import EncoderRegressor\n",
    "  from models.Pix2pix import Generator\n",
    "  from models.SHL_DNN import SHLNeuralNetwork\n",
    "  from models.SwinT import SwinUNet\n",
    "  from models.TM import TransmissionMatrix\n",
    "  from models.U_Net import UNet\n",
    "\n",
    "  from functools import partial\n",
    "  from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "  extract_beam_parameters = partial(extract_beam_parameters, as_array=False)\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  experiment_name = Path(experiment).name.split(\"-\")[0] \n",
    "  dir_yaml = Path(experiment) / (experiment_name.split(\"-\")[0] + \".yaml\")\n",
    "  config_manager = ConfigManager(load_config(exp_name=dir_yaml, prior_machine=True))\n",
    "  config = config_manager.get()\n",
    "  if batch_size is not None: # fix and unify batch size for evaluation\n",
    "      config[\"data\"][\"dataset_ops\"][0]['params']['batch_size'] = batch_size \n",
    "\n",
    "  # ==================== \n",
    "  # Prepare Dataset\n",
    "  # ====================\n",
    "  if change_val_dir is not None:\n",
    "      config[\"data\"][\"evaluation_set\"] = change_val_dir\n",
    "      config[\"data\"][\"train_val_split\"] = 0.0\n",
    "      config[\"data\"][\"val_test_split\"] = 0.0\n",
    "  evaluation_folder = os.path.join(config['paths']['datasets']['mmf'], config[\"data\"][\"evaluation_set\"])\n",
    "  evaluation_provider = FileProvider(evaluation_folder).\\\n",
    "      subsample(fraction=config[\"data\"][\"subsample_fraction\"], seed=config[\"seed\"]) \n",
    "  val_provider, test_provider = evaluation_provider.\\\n",
    "      split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "  transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "  def make_dataset(provider):\n",
    "      return PyTorchPipeline(provider, transforms).to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "\n",
    "  test_dataset = make_dataset(test_provider)\n",
    "  print(\"Samples: \", len(test_provider))\n",
    "  print(\"Batch: \", len(test_dataset))\n",
    "  print(f'processing: {experiment_name}, model left to evaluate: {count+1}/{len(experiments)}')\n",
    "\n",
    "  if experiment_name == \"CAE\":\n",
    "      model = Autoencoder2D.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"img2img\"\n",
    "  elif experiment_name == \"ERN\":\n",
    "      model = EncoderRegressor.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"regression\"\n",
    "  elif experiment_name == \"Pix2pix\":\n",
    "      model = Generator.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"img2img\"\n",
    "  elif experiment_name == \"SHL_DNN\":\n",
    "      model = SHLNeuralNetwork.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"flattened\"\n",
    "  elif experiment_name == \"SwinT\":\n",
    "      model = SwinUNet.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"img2img\"\n",
    "  elif experiment_name == \"TM\":\n",
    "      model = TransmissionMatrix.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"flattened\"\n",
    "  elif experiment_name == \"U_Net\":\n",
    "      model = UNet.load_model(experiment + '/model.pth', device=device)\n",
    "      mode = \"img2img\"\n",
    "\n",
    "  return {\n",
    "          'model': model, 'test_dataset': test_dataset, \n",
    "          'device': device, 'extract_beam_parameters': extract_beam_parameters, \n",
    "          'mode': mode, 'experiment_name': experiment_name, 'experiment_id': str(Path(experiment).name),\n",
    "          'config': config\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27415a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Model evaluation on the test set'''\n",
    "\n",
    "experiments = list_subfolders_abs('results/HPC/')\n",
    "[print(i) for i in experiments]\n",
    "\n",
    "for count, experiment in enumerate(experiments):\n",
    "    # loop over each results\n",
    "    read_outs = fixed_process(experiments, experiment, count)\n",
    "    # 1. get pure prediction results with ground truth as a clear table\n",
    "    read_outs['model'].eval()  # Set to evaluation mode\n",
    "    df = evaluate_to_df(\n",
    "        model=read_outs['model'],\n",
    "        test_loader=read_outs['test_dataset'],\n",
    "        device=read_outs['device'],\n",
    "        extract_beam_parameters=read_outs['extract_beam_parameters'],\n",
    "        mode=read_outs['mode'],\n",
    "    )\n",
    "    # 2. add futher calculation to the table (just do it inmemory)\n",
    "    thresholds = {\"MAE\": 0.05, \"RMSE\": 0.05, \"MSE\": 0.05}\n",
    "    df = add_beamparam_metrics(df)\n",
    "    df = add_extraction_state_and_thresholds(df, thresholds)\n",
    "    df.to_csv(\"results/metrics/{}.csv\".format(Path(experiment).name), index=False)\n",
    "    summarize_error_columns_to_json(df, \"results/metrics/{}_summary.json\".format(Path(experiment).name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tiny quick sanity check'''\n",
    "df = pd.read_csv(\"results/metrics/CAE-20250825044452.csv\")\n",
    "# df = pd.read_csv(\"results/metrics/CAE-20250825064523.csv\")\n",
    "plot_sanity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e9e72",
   "metadata": {},
   "source": [
    "# FIG-1 Training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all the training information into a single folder\n",
    "collect_and_copy_by_keyword(src_dir=\"results/HPC/\", dst_dir=\"results/metrics\", keyword=\"history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  \"train_loss\",\n",
    "  # \"val_loss\",\n",
    "  \"val_overall_mae\",\n",
    "  \"val_overall_mse\",\n",
    "  \"val_overall_rmse\"\n",
    "]\n",
    "\n",
    "ratio = 2\n",
    "set_aps_single_column(figsize=(3.25*ratio, 2.5*ratio), scale=1.4)\n",
    "for m in metrics:\n",
    "    plot_history_curves(folder='results/metrics', \n",
    "                        metrics=m, \n",
    "                        out_dir=\"results/plots\",\n",
    "                        use_line_styles=True, \n",
    "                        show_grid=False, \n",
    "                        line_width=2,  # 1.8\n",
    "                        advanced_smooth=5,\n",
    "                        # epoch_range=[0, 10],\n",
    "                        show_legend=True,\n",
    "                        show_small_ticks=False,\n",
    "                        legend_fontsize=10,\n",
    "                        frame_linewidth=1.0,\n",
    "                        )\n",
    "    # plot_history_curves(folder='results/metrics', metrics=m, out_dir=\"results/plots\", epoch_range=[0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700711ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  \"train_loss\",\n",
    "  # \"val_loss\",\n",
    "  \"val_overall_mae\",\n",
    "  \"val_overall_mse\",\n",
    "  \"val_overall_rmse\"\n",
    "]\n",
    "\n",
    "ratio = 2\n",
    "set_aps_single_column(figsize=(2.5*ratio, 2.5*ratio), scale=2)\n",
    "for m in metrics:\n",
    "    plot_history_curves(folder='results/metrics', \n",
    "                        metrics=m, \n",
    "                        out_dir=\"results/plots\",\n",
    "                        use_line_styles=True, \n",
    "                        show_grid=False, \n",
    "                        line_width=3.4,  # 1.8\n",
    "                        advanced_smooth=5,\n",
    "                        epoch_range=[0, 8],\n",
    "                        show_legend=False,\n",
    "                        show_small_ticks=False,\n",
    "                        legend_fontsize=11,\n",
    "                        frame_linewidth=1.5,\n",
    "                        figsize=(6, 5)\n",
    "                        )\n",
    "    # plot_history_curves(folder='results/metrics', metrics=m, out_dir=\"results/plots\", epoch_range=[0, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6ae59",
   "metadata": {},
   "source": [
    "# FIG-2 Reconstruction image samples\n",
    "Potentially make it beam parameter addable on the image and all individual images? more flexibiilty later for sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6225254",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list_subfolders_abs('results/HPC/')\n",
    "batch_size = 1  # for visualization, set batch size to 1\n",
    "\n",
    "for count, experiment in enumerate(experiments):\n",
    "    if str(Path(experiment).name.split(\"-\")[0]) == \"ERN\": continue  # skip regression model\n",
    "    # loop over each results\n",
    "    read_outs = fixed_process(experiments, experiment, count, batch_size=batch_size, change_val_dir=\"plot_sample/\")\n",
    "    read_outs['model'].eval()  # Set to evaluation mode\n",
    "    selected_samples = list(range(0, len(read_outs['test_dataset']), 1))\n",
    "    for i, (x, y) in enumerate(read_outs['test_dataset']): \n",
    "        if i not in selected_samples:\n",
    "            continue\n",
    "        x, y = x[0], y[0]\n",
    "        save_single_sample_triplet(\n",
    "            model=read_outs['model'],\n",
    "            x=x,    # (C,H,W) or (1,C,H,W)\n",
    "            y=y,    # (C,H,W) or (1,C,H,W)\n",
    "            device=read_outs['device'],\n",
    "            out_dir=\"results/samples/\",\n",
    "            prefix=f\"{read_outs['experiment_id']}_{i}\",\n",
    "            channel=0,\n",
    "            mode=read_outs['mode'],\n",
    "            add_edge_hist=True,\n",
    "            add_centroid=False,\n",
    "            use_gaussian_fit=False,\n",
    "            edge_line_width=5,\n",
    "            edge_hist_scale=0.15,\n",
    "            # vmin=0,\n",
    "            # vmax=1,\n",
    "        )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17325",
   "metadata": {},
   "source": [
    "# FIG-3 Final score box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  # \"h_centroid_MSE\",\n",
    "  # \"h_centroid_RMSE\",\n",
    "  # \"h_centroid_MAE\",\n",
    "  # \"v_centroid_MSE\",\n",
    "  # \"v_centroid_RMSE\",\n",
    "  # \"v_centroid_MAE\",\n",
    "  # \"h_width_MSE\",\n",
    "  # \"h_width_RMSE\",\n",
    "  # \"h_width_MAE\",\n",
    "  # \"v_width_MSE\",\n",
    "  # \"v_width_RMSE\",\n",
    "  # \"v_width_MAE\",\n",
    "  \"RMSE_mean\",\n",
    "  \"MSE_mean\",\n",
    "  \"MAE_mean\",\n",
    "]\n",
    "\n",
    "ratio = 2\n",
    "set_aps_single_column(figsize=(3.25*ratio, 2.5*ratio), scale=1.3)\n",
    "for m in metrics:\n",
    "    plot_metric_box_by_model(\"results/metrics\", m, \"results/plots\", show_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c12d0",
   "metadata": {},
   "source": [
    "# FIG-4 Bar chart over all model final metric scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  # \"h_centroid_MSE\",\n",
    "  # \"h_centroid_RMSE\",\n",
    "  # \"h_centroid_MAE\",\n",
    "  # \"v_centroid_MSE\",\n",
    "  # \"v_centroid_RMSE\",\n",
    "  # \"v_centroid_MAE\",\n",
    "  # \"h_width_MSE\",\n",
    "  # \"h_width_RMSE\",\n",
    "  # \"h_width_MAE\",\n",
    "  # \"v_width_MSE\",\n",
    "  # \"v_width_RMSE\",\n",
    "  # \"v_width_MAE\",\n",
    "  \"MSE_mean\",\n",
    "  \"RMSE_mean\",\n",
    "  \"MAE_mean\",\n",
    "]\n",
    "\n",
    "ratio = 2\n",
    "set_aps_single_column(figsize=(3.25*ratio, 2.5*ratio), scale=1.5)\n",
    "statistics = plot_metrics_grouped_bars_by_model(\"results/metrics\", \n",
    "                                                metrics, \"results/plots\",\n",
    "                                                with_std=False,\n",
    "                                                sort_by_mae=True,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be19e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f834043",
   "metadata": {},
   "source": [
    "# FIG-5 Per model testset error box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f600183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: results/plots/MAE_mean_by_model_boxplot.pdf\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "  # \"MSE_mean\",\n",
    "  # \"RMSE_mean\",\n",
    "  \"MAE_mean\",\n",
    "]\n",
    "\n",
    "ratio = 2\n",
    "set_aps_single_column(figsize=(3.25*ratio, 2.5*ratio), scale=1.3)\n",
    "\n",
    "for m in metrics:\n",
    "  plot_metric_box_by_model_csv(\n",
    "    root_dir='results/metrics', \n",
    "    metric=m, \n",
    "    out_dir=f'results/plots/',\n",
    "    show_grid=True,\n",
    "    spacing=1,\n",
    "    show_violin=False,\n",
    "    show_hist=True,\n",
    "    overlay_clip_max=0.1,\n",
    "    hist_bins=200,\n",
    "    box_width=0.25, # 0.45\n",
    "    hist_alpha=0.9, # 0.25\n",
    "    sort_by_mae=True,\n",
    "    figsize=(7, 4),\n",
    "  )\n",
    "\n",
    "    # plot_model_error_boxplot(input_dir='results/metrics', \n",
    "    #                          metric=m, \n",
    "    #                          output_pdf=f'results/plots/{m}_boxplot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba1dab",
   "metadata": {},
   "source": [
    "# Parse Slurm training history files to get model size and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_training_logs('results/slurm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bcf60",
   "metadata": {},
   "source": [
    "# Fix Model bug (did not save the correct model setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify the model directory\n",
    "model_dir = '/Users/andrewxu/Documents/GitHub/fiber-image-reconstruction-comparison/results/HPC/SwinT-20250901143642'\n",
    "model_path = Path(model_dir) / 'model.pth'\n",
    "\n",
    "# Get experiment name and corresponding YAML file\n",
    "experiment_name = Path(model_dir).name.split(\"-\")[0]\n",
    "yaml_path = f\"{model_dir}/SwinT.yaml\"\n",
    "\n",
    "print(f\"Processing: {experiment_name}\")\n",
    "print(f\"Loading config from: {yaml_path}\")\n",
    "\n",
    "# Load the model checkpoint\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "# Load the YAML config\n",
    "with open(yaml_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "model_config = config['model']\n",
    "model_config\n",
    "\n",
    "print(f\"Model config from YAML: {model_config}\")\n",
    "\n",
    "# Update the checkpoint with the correct config\n",
    "ckpt['config'] = model_config\n",
    "\n",
    "# Save back to the same location\n",
    "torch.save(ckpt, model_path)\n",
    "print(f\"âœ… Updated checkpoint saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
