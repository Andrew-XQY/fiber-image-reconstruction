{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed5cdf6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79336363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())            # check\n",
    "\n",
    "# pip install xflow-py\n",
    "from xflow import ConfigManager, SqlProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.utils import load_validated_config, save_image\n",
    "import xflow.extensions.physics\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import tarfile\n",
    "from datetime import datetime  \n",
    "from config_utils import load_config\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Create experiment output directory  (timestamped)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")  \n",
    "\n",
    "experiment_name = \"CAE_validate_clear\"  # TM, SHL_DNN, U_Net, Pix2pix, ERN, CAE, SwinT, CAE_syth\n",
    "model_name = \"CAE\"\n",
    "folder_name = f\"{experiment_name}-{timestamp}\"  \n",
    "config_manager = ConfigManager(load_config(f\"{experiment_name}.yaml\", \n",
    "                                           experiment_name=folder_name))\n",
    "config = config_manager.get()\n",
    "config_manager.add_files(config[\"extra_files\"])\n",
    "\n",
    "experiment_output_dir = config[\"paths\"][\"output\"]\n",
    "os.makedirs(experiment_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# New structure, read the database table first, get files from it.\n",
    "# Extract tar file if needed\n",
    "if config['file_extract']:\n",
    "    dataset_tar_file = os.path.join(config[\"paths\"][\"dataset\"], config[\"data\"][\"dataset\"])\n",
    "    dataset_base_dir = os.path.dirname(dataset_tar_file)\n",
    "    dataset_name = os.path.splitext(config[\"data\"][\"dataset\"])[0]  # Remove .tar extension\n",
    "    dataset_extracted_dir = os.path.join(dataset_base_dir, dataset_name)\n",
    "\n",
    "    # Unzip tar file if not already extracted\n",
    "    if not os.path.exists(dataset_extracted_dir):\n",
    "        print(f\"Extracting {dataset_tar_file}...\")\n",
    "        with tarfile.open(dataset_tar_file, 'r') as tar:\n",
    "            tar.extractall(path=dataset_base_dir)\n",
    "        print(f\"Extracted to {dataset_extracted_dir}\")\n",
    "    else:\n",
    "        print(f\"Dataset already extracted at {dataset_extracted_dir}\")\n",
    "        \n",
    "\n",
    "def make_dataset(provider, transforms):\n",
    "    pipeline = PyTorchPipeline(provider, transforms)\n",
    "    dataset = pipeline.to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "    return dataset, pipeline.in_memory_sample_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434c04d",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (Laser scan + YAG screen )\n",
    "# ====================\n",
    "train_dir = config[\"paths\"][\"training_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{train_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\" \n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (15)\n",
    "--LIMIT 300\n",
    "\"\"\"\n",
    "train_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "\n",
    "test_dir = config[\"paths\"][\"test_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (1, 7)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "evaluation_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# Swap traing dataset and evaluation dataset\n",
    "train_dir, test_dir = test_dir, train_dir\n",
    "train_provider, evaluation_provider = evaluation_provider, train_provider\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": train_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_datase, n1 = make_dataset(train_provider, transforms)\n",
    "\n",
    "# For test datasets\n",
    "config[\"data\"][\"transforms\"][\"torch\"][0][\"params\"][\"parent_dir\"] = test_dir\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "val_dataset, n2 = make_dataset(val_provider, transforms)\n",
    "test_dataset, n3 = make_dataset(test_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (YAG screen)\n",
    "# ====================\n",
    "\n",
    "test_dir = config[\"paths\"][\"test_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (1, 7)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "train_provider, evaluation_provider = realbeam_provider.split(ratio=config[\"data\"][\"train_val_split\"], seed=config[\"seed\"])\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": test_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_dataset, n1 = make_dataset(train_provider, transforms)\n",
    "val_dataset, n2 = make_dataset(val_provider, transforms)\n",
    "test_dataset, n3 = make_dataset(test_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78860ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (Friday + Saturday Chromox)\n",
    "# ====================\n",
    "\n",
    "test_dir = config[\"paths\"][\"\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (10, 11, 12)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "train_provider, evaluation_provider = realbeam_provider.split(ratio=config[\"data\"][\"train_val_split\"], seed=config[\"seed\"])\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": test_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_dataset, n1 = make_dataset(train_provider, transforms)\n",
    "val_dataset, n2 = make_dataset(val_provider, transforms)\n",
    "test_dataset, n3 = make_dataset(test_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples in providers: \",len(train_provider),len(val_provider),len(test_provider))\n",
    "print(\"Total samples in datasets:\", n1, n2, n3)\n",
    "print(\"Batch: \",len(train_dataset),len(val_dataset),len(test_dataset))\n",
    "\n",
    "# save a sample from dataset for debugging\n",
    "if model_name in REGRESSION:\n",
    "    for left_parts, params, right_parts in test_dataset:\n",
    "        print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "        save_image(left_parts[0], config[\"paths\"][\"output\"] + \"/input.png\")\n",
    "        save_image(right_parts[0], config[\"paths\"][\"output\"] + \"/output.png\")\n",
    "        break\n",
    "else:\n",
    "    for index, sample in enumerate(test_dataset):  # test_dataset\n",
    "        left_parts, right_parts = sample\n",
    "        # batch will be a tuple: (right_halves, left_halves) due to split_width\n",
    "        print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "        if model_name in SAMPLE_FLATTENED:\n",
    "            save_image(left_parts[0].reshape(config['data']['input_shape']), config[\"paths\"][\"output\"] + f\"/input_{index}.png\")\n",
    "            save_image(right_parts[0].reshape(config['data']['output_shape']), config[\"paths\"][\"output\"] + f\"/output_{index}.png\")\n",
    "        else:\n",
    "            save_image(left_parts[0], config[\"paths\"][\"output\"] + f\"/input_{index}.png\")\n",
    "            save_image(right_parts[0], config[\"paths\"][\"output\"] + f\"/output_{index}.png\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436c74c",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Construct Model\n",
    "# ====================\n",
    "if model_name == \"CAE\":\n",
    "    from models.CAE import Autoencoder2D\n",
    "    model = Autoencoder2D(\n",
    "        in_channels=int(config['model'][\"in_channels\"]),\n",
    "        encoder=config['model'][\"encoder\"],\n",
    "        decoder=config['model'][\"decoder\"],\n",
    "        kernel_size=int(config['model'][\"kernel_size\"]),\n",
    "        apply_batchnorm=config['model'][\"apply_batchnorm\"],\n",
    "        apply_dropout=config['model'][\"apply_dropout\"],\n",
    "        final_activation=str(config['model'][\"final_activation\"]),\n",
    "    )\n",
    "elif model_name == \"TM\":\n",
    "    from models.TM import TransmissionMatrix\n",
    "    model = TransmissionMatrix(\n",
    "        input_height = config[\"data\"][\"input_shape\"][0],\n",
    "        input_width = config[\"data\"][\"input_shape\"][1],\n",
    "        output_height = config[\"data\"][\"output_shape\"][0],\n",
    "        output_width = config[\"data\"][\"output_shape\"][1],\n",
    "        initialization = \"xavier\",\n",
    "    )\n",
    "elif model_name == \"SHL_DNN\":\n",
    "    from models.SHL_DNN import SHLNeuralNetwork\n",
    "    model = SHLNeuralNetwork(\n",
    "        input_size=config['data']['input_shape'][0] * config['data']['input_shape'][1],\n",
    "        hidden_size=config['model']['hidden_size'], \n",
    "        output_size=config['data']['output_shape'][0] * config['data']['output_shape'][1],\n",
    "        dropout_rate=config['model']['dropout_rate'],\n",
    "    )\n",
    "elif model_name == \"U_Net\":\n",
    "    from models.U_Net import UNet\n",
    "    model = UNet(\n",
    "        in_channels=config[\"model\"][\"in_channels\"],\n",
    "        encoder=config[\"model\"][\"encoder\"],\n",
    "        decoder=config[\"model\"][\"decoder\"],\n",
    "        kernel_size=config[\"model\"][\"kernel_size\"],\n",
    "        apply_batchnorm=config[\"model\"][\"apply_batchnorm\"],\n",
    "        apply_dropout=config[\"model\"][\"apply_dropout\"],\n",
    "        out_channels=config[\"model\"][\"out_channels\"],\n",
    "        final_activation=config[\"model\"][\"final_activation\"],\n",
    "    )\n",
    "elif model_name == \"SwinT\":\n",
    "    from models.SwinT import SwinUNet, ReconLoss\n",
    "    model = SwinUNet(\n",
    "        img_size=config['model']['img_size'],\n",
    "        in_chans=config['model']['in_chans'],\n",
    "        out_chans=config['model']['out_chans'],\n",
    "        embed_dim=config['model']['embed_dim'],\n",
    "        depths=config['model']['depths'],\n",
    "        num_heads=config['model']['num_heads'],\n",
    "        window_size=config['model']['window_size'],\n",
    "        patch_size=config['model']['patch_size'],\n",
    "    )\n",
    "elif model_name == \"Pix2pix\":\n",
    "    from models.Pix2pix import Generator, Discriminator, Pix2PixLosses\n",
    "    G = Generator(channels=config[\"model\"][\"channels\"])\n",
    "    D = Discriminator(channels=config[\"model\"][\"channels\"])\n",
    "    losses = Pix2PixLosses(lambda_l1=config[\"model\"][\"lambda_l1\"])\n",
    "    opt_g = torch.optim.Adam(G.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=config[\"training\"][\"betas\"])\n",
    "    opt_d = torch.optim.Adam(D.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=config[\"training\"][\"betas\"])\n",
    "elif model_name == \"ERN\":\n",
    "    from models.ERN import EncoderRegressor\n",
    "    model = EncoderRegressor(\n",
    "            in_channels=config['model']['in_channels'],\n",
    "            kernel_size=config['model']['kernel_size'],\n",
    "            encoder=config['model']['encoder'],\n",
    "            decoder=config['model']['decoder'],\n",
    "            final_activation=config['model']['final_activation'],  \n",
    "        )\n",
    "elif model_name == \"CAE_syth\":\n",
    "    from models.CAE import Autoencoder2D\n",
    "    model = Autoencoder2D(\n",
    "        in_channels=int(config['model'][\"in_channels\"]),\n",
    "        encoder=config['model'][\"encoder\"],\n",
    "        decoder=config['model'][\"decoder\"],\n",
    "        kernel_size=int(config['model'][\"kernel_size\"]),\n",
    "        apply_batchnorm=config['model'][\"apply_batchnorm\"],\n",
    "        apply_dropout=config['model'][\"apply_dropout\"],\n",
    "        final_activation=str(config['model'][\"final_activation\"]),\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if model_name == \"Pix2pix\":\n",
    "    G = G.to(device)\n",
    "    D = D.to(device)\n",
    "    show_model_info(G)\n",
    "    show_model_info(D)\n",
    "elif model_name == \"SwinT\":\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_steps = config['training']['epochs'] * len(train_dataset)\n",
    "    warmup_steps = int(config['training']['warmup_ratio'] * total_steps)\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = ReconLoss(w_l1=config['training']['w_l1'], w_ssim=config['training']['w_ssim']) # Loss: L1 + 0.3*SSIM\n",
    "\n",
    "    # Optimizer: AdamW with recommended params\n",
    "    base_lr = 4e-4 if config['training']['batch_size'] >= 64 else 2e-4\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=base_lr, betas=config['training']['betas'],\n",
    "        eps=config['training']['eps'], weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    show_model_info(model)\n",
    "else:\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "    show_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Training\n",
    "# ====================\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "\n",
    "from xflow import TorchTrainer, TorchGANTrainer\n",
    "from xflow.trainers import build_callbacks_from_config\n",
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "# 1) loss/optimizer\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "# loss function with weighting on high-intensity regions\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, alpha=5.0, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        weight = 1.0 + self.alpha * (gt.clamp(min=0.) ** self.beta)\n",
    "        return (weight * (pred - gt) ** 2).mean()\n",
    "\n",
    "criterion = WeightedMSELoss(alpha=5.0, beta=0.5)\n",
    "\n",
    "# 2) callbacks (unchanged) + any custom wiring\n",
    "callbacks = build_callbacks_from_config(\n",
    "    config=config[\"callbacks\"],\n",
    "    framework=config[\"framework\"],  \n",
    ") # keep dataset closure for last callback, sequence hardcoded\n",
    "callbacks[-1].set_dataset(test_dataset)\n",
    "\n",
    "# Extract beam parameters closure (return as dict)\n",
    "if model_name in SAMPLE_FLATTENED:\n",
    "    extract_beam_parameters_dict = partial(extract_beam_parameters_flat, as_array=False)\n",
    "    beam_param_metric = make_beam_param_metric(extract_beam_parameters_dict)\n",
    "elif model_name in REGRESSION:   # e.g., \"ERN\"\n",
    "    beam_param_metric = make_param_metric()\n",
    "else:\n",
    "    extract_beam_parameters_dict = partial(extract_beam_parameters, as_array=False)\n",
    "    beam_param_metric = make_beam_param_metric(extract_beam_parameters_dict)\n",
    "\n",
    "# 3) run training\n",
    "if model_name in GAN:\n",
    "    trainer = TorchGANTrainer(\n",
    "        generator=G,\n",
    "        discriminator=D,\n",
    "        optimizer_g=opt_g,\n",
    "        optimizer_d=opt_d,\n",
    "        losses=losses,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        output_dir=config[\"paths\"][\"output\"],\n",
    "        data_pipeline=train_dataset,\n",
    "        val_metrics=[beam_param_metric],\n",
    "    )\n",
    "else:\n",
    "    trainer = TorchTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        output_dir=config[\"paths\"][\"output\"],\n",
    "        data_pipeline=train_dataset,\n",
    "        val_metrics=[beam_param_metric],\n",
    "        scheduler= scheduler if model_name == \"SwinT\" else None, \n",
    "        scheduler_step_per_batch=True,\n",
    "    )\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=train_dataset, \n",
    "    val_loader=val_dataset,\n",
    "    epochs=config['training']['epochs'],\n",
    ")\n",
    "\n",
    "# 4) save results\n",
    "trainer.save_history(f\"{config['paths']['output']}/history.json\")\n",
    "trainer.save_model(config[\"paths\"][\"output\"])  # uses model.save_model(...) if available\n",
    "config_manager.save(output_dir=config[\"paths\"][\"output\"], config_filename=config[\"name\"])\n",
    "\n",
    "print(\"Training ALL complete.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed03c6",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fec9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"/Users/andrewxu/Desktop/untitled folder 2\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "def _save_triplet(idx, inp_tensor, gt_tensor, pred_tensor):\n",
    "    torch.save(\n",
    "        {\"input\": inp_tensor, \"ground_truth\": gt_tensor, \"prediction\": pred_tensor},\n",
    "        save_dir / f\"sample_{idx:05d}.pt\",\n",
    "    )\n",
    "    save_image(inp_tensor[0], str(save_dir / f\"input_{idx:05d}.png\"))\n",
    "    save_image(gt_tensor[0], str(save_dir / f\"ground_truth_{idx:05d}.png\"))\n",
    "    save_image(pred_tensor[0], str(save_dir / f\"prediction_{idx:05d}.png\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    if model_name in REGRESSION:\n",
    "        for idx, (left_parts, params, right_parts) in enumerate(test_dataset):\n",
    "            inputs = left_parts.to(device)\n",
    "            targets = right_parts.to(device)\n",
    "            params = params.to(device) if torch.is_tensor(params) else params\n",
    "            preds = model(inputs, params).cpu()\n",
    "\n",
    "            _save_triplet(idx, inputs.cpu(), targets.cpu(), preds)\n",
    "    else:\n",
    "        for idx, (left_parts, right_parts) in enumerate(test_dataset):\n",
    "            inputs = left_parts.to(device)\n",
    "            targets = right_parts.to(device)\n",
    "            preds = model(inputs).cpu()\n",
    "\n",
    "            if model_name in SAMPLE_FLATTENED:\n",
    "                inp = inputs.cpu()\n",
    "                tgt = targets.cpu()\n",
    "                pred = preds\n",
    "                inp[0] = inp[0].reshape(*config[\"data\"][\"input_shape\"])\n",
    "                tgt[0] = tgt[0].reshape(*config[\"data\"][\"output_shape\"])\n",
    "                pred[0] = pred[0].reshape(*config[\"data\"][\"output_shape\"])\n",
    "                _save_triplet(idx, inp, tgt, pred)\n",
    "            else:\n",
    "                _save_triplet(idx, inputs.cpu(), targets.cpu(), preds)\n",
    "\n",
    "print(f\"Saved predictions to {save_dir}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
