{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed5cdf6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79336363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())            # check\n",
    "\n",
    "# pip install xflow-py\n",
    "from xflow import ConfigManager, SqlProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.utils import load_validated_config, save_image\n",
    "import xflow.extensions.physics\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import tarfile\n",
    "from datetime import datetime  \n",
    "from config_utils import load_config\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Create experiment output directory  (timestamped)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")  \n",
    "\n",
    "experiment_name = \"CAE\"  # TM, SHL_DNN, U_Net, Pix2pix, ERN, CAE, SwinT, CAE_syth\n",
    "folder_name = f\"{experiment_name}-{timestamp}\"  \n",
    "config_manager = ConfigManager(load_config(f\"{experiment_name}.yaml\", \n",
    "                                           experiment_name=folder_name))\n",
    "config = config_manager.get()\n",
    "config_manager.add_files(config[\"extra_files\"])\n",
    "\n",
    "experiment_output_dir = config[\"paths\"][\"output\"]\n",
    "os.makedirs(experiment_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# New structure, read the database table first, get files from it.\n",
    "# Extract tar file if needed\n",
    "if config['file_extract']:\n",
    "    dataset_tar_file = os.path.join(config[\"paths\"][\"dataset\"], config[\"data\"][\"dataset\"])\n",
    "    dataset_base_dir = os.path.dirname(dataset_tar_file)\n",
    "    dataset_name = os.path.splitext(config[\"data\"][\"dataset\"])[0]  # Remove .tar extension\n",
    "    dataset_extracted_dir = os.path.join(dataset_base_dir, dataset_name)\n",
    "\n",
    "    # Unzip tar file if not already extracted\n",
    "    if not os.path.exists(dataset_extracted_dir):\n",
    "        print(f\"Extracting {dataset_tar_file}...\")\n",
    "        with tarfile.open(dataset_tar_file, 'r') as tar:\n",
    "            tar.extractall(path=dataset_base_dir)\n",
    "        print(f\"Extracted to {dataset_extracted_dir}\")\n",
    "    else:\n",
    "        print(f\"Dataset already extracted at {dataset_extracted_dir}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434c04d",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset\n",
    "# ====================\n",
    "\n",
    "def make_dataset(provider, transforms):\n",
    "    return PyTorchPipeline(provider, transforms, enable_validation=True).to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "\n",
    "train_dir = config[\"paths\"][\"training_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{train_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (15)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "train_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "\n",
    "test_dir = config[\"paths\"][\"test_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (1, 7)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "evaluation_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": train_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_dataset = make_dataset(train_provider, transforms)\n",
    "\n",
    "# For test datasets\n",
    "config[\"data\"][\"transforms\"][\"torch\"][0][\"params\"][\"parent_dir\"] = test_dir\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "val_dataset = make_dataset(val_provider, transforms)\n",
    "test_dataset = make_dataset(test_provider, transforms)\n",
    "\n",
    "print(\"Total Samples in providers: \",len(train_provider),len(val_provider),len(test_provider))\n",
    "print(\"Total Samples in datasets: \",len(train_dataset)+len(val_dataset)+len(test_dataset))\n",
    "print(\"Batch: \",len(train_dataset),len(val_dataset),len(test_dataset))\n",
    "\n",
    "# save a sample from dataset for debugging\n",
    "if experiment_name in REGRESSION:\n",
    "    for left_parts, params, right_parts in test_dataset:\n",
    "        print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "        save_image(left_parts[0], config[\"paths\"][\"output\"] + \"/input.png\")\n",
    "        save_image(right_parts[0], config[\"paths\"][\"output\"] + \"/output.png\")\n",
    "        break\n",
    "else:\n",
    "    for index, sample in enumerate(test_dataset):  # test_dataset\n",
    "        left_parts, right_parts = sample\n",
    "        # batch will be a tuple: (right_halves, left_halves) due to split_width\n",
    "        print(f\"Batch shapes: {left_parts.shape}, {right_parts.shape}\")\n",
    "        if experiment_name in SAMPLE_FLATTENED:\n",
    "            save_image(left_parts[0].reshape(config['data']['input_shape']), config[\"paths\"][\"output\"] + f\"/input_{index}.png\")\n",
    "            save_image(right_parts[0].reshape(config['data']['output_shape']), config[\"paths\"][\"output\"] + f\"/output_{index}.png\")\n",
    "        else:\n",
    "            save_image(left_parts[0], config[\"paths\"][\"output\"] + f\"/input_{index}.png\")\n",
    "            save_image(right_parts[0], config[\"paths\"][\"output\"] + f\"/output_{index}.png\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436c74c",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Construct Model\n",
    "# ====================\n",
    "if experiment_name == \"CAE\":\n",
    "    from models.CAE import Autoencoder2D\n",
    "    model = Autoencoder2D(\n",
    "        in_channels=int(config['model'][\"in_channels\"]),\n",
    "        encoder=config['model'][\"encoder\"],\n",
    "        decoder=config['model'][\"decoder\"],\n",
    "        kernel_size=int(config['model'][\"kernel_size\"]),\n",
    "        apply_batchnorm=config['model'][\"apply_batchnorm\"],\n",
    "        apply_dropout=config['model'][\"apply_dropout\"],\n",
    "        final_activation=str(config['model'][\"final_activation\"]),\n",
    "    )\n",
    "elif experiment_name == \"TM\":\n",
    "    from models.TM import TransmissionMatrix\n",
    "    model = TransmissionMatrix(\n",
    "        input_height = config[\"data\"][\"input_shape\"][0],\n",
    "        input_width = config[\"data\"][\"input_shape\"][1],\n",
    "        output_height = config[\"data\"][\"output_shape\"][0],\n",
    "        output_width = config[\"data\"][\"output_shape\"][1],\n",
    "        initialization = \"xavier\",\n",
    "    )\n",
    "elif experiment_name == \"SHL_DNN\":\n",
    "    from models.SHL_DNN import SHLNeuralNetwork\n",
    "    model = SHLNeuralNetwork(\n",
    "        input_size=config['data']['input_shape'][0] * config['data']['input_shape'][1],\n",
    "        hidden_size=config['model']['hidden_size'], \n",
    "        output_size=config['data']['output_shape'][0] * config['data']['output_shape'][1],\n",
    "        dropout_rate=config['model']['dropout_rate'],\n",
    "    )\n",
    "elif experiment_name == \"U_Net\":\n",
    "    from models.U_Net import UNet\n",
    "    model = UNet(\n",
    "        in_channels=config[\"model\"][\"in_channels\"],\n",
    "        encoder=config[\"model\"][\"encoder\"],\n",
    "        decoder=config[\"model\"][\"decoder\"],\n",
    "        kernel_size=config[\"model\"][\"kernel_size\"],\n",
    "        apply_batchnorm=config[\"model\"][\"apply_batchnorm\"],\n",
    "        apply_dropout=config[\"model\"][\"apply_dropout\"],\n",
    "        out_channels=config[\"model\"][\"out_channels\"],\n",
    "        final_activation=config[\"model\"][\"final_activation\"],\n",
    "    )\n",
    "elif experiment_name == \"SwinT\":\n",
    "    from models.SwinT import SwinUNet, ReconLoss\n",
    "    model = SwinUNet(\n",
    "        img_size=config['model']['img_size'],\n",
    "        in_chans=config['model']['in_chans'],\n",
    "        out_chans=config['model']['out_chans'],\n",
    "        embed_dim=config['model']['embed_dim'],\n",
    "        depths=config['model']['depths'],\n",
    "        num_heads=config['model']['num_heads'],\n",
    "        window_size=config['model']['window_size'],\n",
    "        patch_size=config['model']['patch_size'],\n",
    "    )\n",
    "elif experiment_name == \"Pix2pix\":\n",
    "    from models.Pix2pix import Generator, Discriminator, Pix2PixLosses\n",
    "    G = Generator(channels=config[\"model\"][\"channels\"])\n",
    "    D = Discriminator(channels=config[\"model\"][\"channels\"])\n",
    "    losses = Pix2PixLosses(lambda_l1=config[\"model\"][\"lambda_l1\"])\n",
    "    opt_g = torch.optim.Adam(G.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=config[\"training\"][\"betas\"])\n",
    "    opt_d = torch.optim.Adam(D.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=config[\"training\"][\"betas\"])\n",
    "elif experiment_name == \"ERN\":\n",
    "    from models.ERN import EncoderRegressor\n",
    "    model = EncoderRegressor(\n",
    "            in_channels=config['model']['in_channels'],\n",
    "            kernel_size=config['model']['kernel_size'],\n",
    "            encoder=config['model']['encoder'],\n",
    "            decoder=config['model']['decoder'],\n",
    "            final_activation=config['model']['final_activation'],  \n",
    "        )\n",
    "elif experiment_name == \"CAE_syth\":\n",
    "    from models.CAE import Autoencoder2D\n",
    "    model = Autoencoder2D(\n",
    "        in_channels=int(config['model'][\"in_channels\"]),\n",
    "        encoder=config['model'][\"encoder\"],\n",
    "        decoder=config['model'][\"decoder\"],\n",
    "        kernel_size=int(config['model'][\"kernel_size\"]),\n",
    "        apply_batchnorm=config['model'][\"apply_batchnorm\"],\n",
    "        apply_dropout=config['model'][\"apply_dropout\"],\n",
    "        final_activation=str(config['model'][\"final_activation\"]),\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if experiment_name == \"Pix2pix\":\n",
    "    G = G.to(device)\n",
    "    D = D.to(device)\n",
    "    show_model_info(G)\n",
    "    show_model_info(D)\n",
    "elif experiment_name == \"SwinT\":\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    total_steps = config['training']['epochs'] * len(train_dataset)\n",
    "    warmup_steps = int(config['training']['warmup_ratio'] * total_steps)\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = ReconLoss(w_l1=config['training']['w_l1'], w_ssim=config['training']['w_ssim']) # Loss: L1 + 0.3*SSIM\n",
    "\n",
    "    # Optimizer: AdamW with recommended params\n",
    "    base_lr = 4e-4 if config['training']['batch_size'] >= 64 else 2e-4\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=base_lr, betas=config['training']['betas'],\n",
    "        eps=config['training']['eps'], weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    show_model_info(model)\n",
    "else:\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "    show_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Training\n",
    "# ====================\n",
    "from functools import partial\n",
    "\n",
    "from xflow import TorchTrainer, TorchGANTrainer\n",
    "from xflow.trainers import build_callbacks_from_config\n",
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "# 1) loss/optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 2) callbacks (unchanged) + any custom wiring\n",
    "callbacks = build_callbacks_from_config(\n",
    "    config=config[\"callbacks\"],\n",
    "    framework=config[\"framework\"],  \n",
    ") # keep dataset closure for last callback, sequence hardcoded\n",
    "callbacks[-1].set_dataset(test_dataset)\n",
    "\n",
    "# Extract beam parameters closure (return as dict)\n",
    "if experiment_name in SAMPLE_FLATTENED:\n",
    "    extract_beam_parameters_dict = partial(extract_beam_parameters_flat, as_array=False)\n",
    "    beam_param_metric = make_beam_param_metric(extract_beam_parameters_dict)\n",
    "elif experiment_name in REGRESSION:   # e.g., \"ERN\"\n",
    "    beam_param_metric = make_param_metric()\n",
    "else:\n",
    "    extract_beam_parameters_dict = partial(extract_beam_parameters, as_array=False)\n",
    "    beam_param_metric = make_beam_param_metric(extract_beam_parameters_dict)\n",
    "\n",
    "# 3) run training\n",
    "if experiment_name in GAN:\n",
    "    trainer = TorchGANTrainer(\n",
    "        generator=G,\n",
    "        discriminator=D,\n",
    "        optimizer_g=opt_g,\n",
    "        optimizer_d=opt_d,\n",
    "        losses=losses,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        output_dir=config[\"paths\"][\"output\"],\n",
    "        data_pipeline=train_dataset,\n",
    "        val_metrics=[beam_param_metric],\n",
    "    )\n",
    "else:\n",
    "    trainer = TorchTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        callbacks=callbacks,\n",
    "        output_dir=config[\"paths\"][\"output\"],\n",
    "        data_pipeline=train_dataset,\n",
    "        val_metrics=[beam_param_metric],\n",
    "        scheduler= scheduler if experiment_name == \"SwinT\" else None, \n",
    "        scheduler_step_per_batch=True,\n",
    "    )\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=train_dataset, \n",
    "    val_loader=val_dataset,\n",
    "    epochs=config['training']['epochs'],\n",
    ")\n",
    "\n",
    "# 4) save results\n",
    "trainer.save_history(f\"{config['paths']['output']}/history.json\")\n",
    "trainer.save_model(config[\"paths\"][\"output\"])  # uses model.save_model(...) if available\n",
    "config_manager.save(output_dir=config[\"paths\"][\"output\"], config_filename=config[\"name\"])\n",
    "\n",
    "print(\"Training ALL complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed03c6",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from xflow.data.transform import add_parent_dir, split_width\n",
    "\n",
    "def compute_intensity_centers(image_paths):\n",
    "    \"\"\"\n",
    "    Given a list of image file paths, compute intensity-weighted\n",
    "    (x, y) centers for each image and return a list of (x, y) tuples.\n",
    "    \"\"\"\n",
    "    centers = []\n",
    "\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img, _ = split_width(img)\n",
    "\n",
    "        # 1D projections along x and y\n",
    "        x_hist = img.sum(axis=0)  # width\n",
    "        y_hist = img.sum(axis=1)  # height\n",
    "\n",
    "        x_idx = np.arange(img.shape[1])\n",
    "        y_idx = np.arange(img.shape[0])\n",
    "\n",
    "        x_sum = x_hist.sum()\n",
    "        y_sum = y_hist.sum()\n",
    "\n",
    "        x_center = (x_hist * x_idx).sum() / x_sum if x_sum > 0 else np.nan\n",
    "        y_center = (y_hist * y_idx).sum() / y_sum if y_sum > 0 else np.nan\n",
    "\n",
    "        centers.append((x_center, y_center))\n",
    "\n",
    "    return centers\n",
    "\n",
    "center_points_testset = compute_intensity_centers([add_parent_dir(x, config[\"paths\"][\"test_set\"]) for x in evaluation_provider()] )\n",
    "center_points_trainset = compute_intensity_centers([add_parent_dir(x, config[\"paths\"][\"training_set\"]) for x in train_provider()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def plot_points_and_fit_square(\n",
    "    points1,\n",
    "    points2=None,\n",
    "    image_size=(1920, 1200),\n",
    "    figsize=(10, 6),\n",
    "    dpi=150,\n",
    "    point_size=5,\n",
    "    label1=\"Set 1\",\n",
    "    label2=\"Set 2\",\n",
    "):\n",
    "    width, height = image_size\n",
    "\n",
    "    # collect all valid points\n",
    "    all_points = []\n",
    "    if points1 is not None:\n",
    "        all_points.extend(points1)\n",
    "    if points2 is not None:\n",
    "        all_points.extend(points2)\n",
    "\n",
    "    all_points = [\n",
    "        (x, y)\n",
    "        for x, y in all_points\n",
    "        if x is not None and y is not None and not (np.isnan(x) or np.isnan(y))\n",
    "    ]\n",
    "\n",
    "    if not all_points:\n",
    "        return None, None\n",
    "\n",
    "    xs = np.array([p[0] for p in all_points], dtype=float)\n",
    "    ys = np.array([p[1] for p in all_points], dtype=float)\n",
    "\n",
    "    min_x, max_x = xs.min(), xs.max()\n",
    "    min_y, max_y = ys.min(), ys.max()\n",
    "\n",
    "    width_bb = max_x - min_x\n",
    "    height_bb = max_y - min_y\n",
    "    side = max(width_bb, height_bb)\n",
    "\n",
    "    cx = (min_x + max_x) / 2.0\n",
    "    cy = (min_y + max_y) / 2.0\n",
    "    half = side / 2.0\n",
    "\n",
    "    tl_x = cx - half\n",
    "    tl_y = cy - half\n",
    "    br_x = cx + half\n",
    "    br_y = cy + half\n",
    "\n",
    "    top_left = (tl_x, tl_y)\n",
    "    bottom_right = (br_x, br_y)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(height, 0)  # invert y to image coords\n",
    "    ax.set_facecolor(\"black\")\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    if points1:\n",
    "        xs1 = [p[0] for p in points1]\n",
    "        ys1 = [p[1] for p in points1]\n",
    "        h1 = ax.scatter(xs1, ys1, s=point_size, c=\"red\", label=label1, marker=\"o\")\n",
    "        handles.append(h1)\n",
    "\n",
    "    if points2:\n",
    "        xs2 = [p[0] for p in points2]\n",
    "        ys2 = [p[1] for p in points2]\n",
    "        h2 = ax.scatter(xs2, ys2, s=point_size, c=\"green\", label=label2, marker=\"o\")\n",
    "        handles.append(h2)\n",
    "\n",
    "    rect = Rectangle(\n",
    "        (tl_x, tl_y),\n",
    "        br_x - tl_x,\n",
    "        br_y - tl_y,\n",
    "        fill=False,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    if handles:\n",
    "        # markerscale multiplies the original marker size\n",
    "        ax.legend(markerscale=20)\n",
    "\n",
    "    plt.show()\n",
    "    return top_left, bottom_right\n",
    "\n",
    "\n",
    "\n",
    "tl, br = plot_points_and_fit_square(\n",
    "    center_points_trainset,\n",
    "    center_points_testset,\n",
    "    image_size=(1920, 1200),\n",
    "    figsize=(12, 7),\n",
    "    point_size=0.01,\n",
    "    label1=\"Laser point scan\",\n",
    "    label2=\"CLEAR YAG screen\",\n",
    "    dpi=200,\n",
    ")\n",
    "\n",
    "print(f\"Fitted square top-left: {tl}, bottom-right: {br}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_and_fit_square(\n",
    "    center_points_testset,\n",
    "    image_size=(1920, 1200),\n",
    "    figsize=(12, 7),\n",
    "    point_size=0.01,\n",
    "    label1=\"Laser point scan\",\n",
    "    label2=\"CLEAR YAG screen\",\n",
    "    dpi=200,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
