{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801323da",
   "metadata": {},
   "source": [
    "# Dataset Statistics/exploration\n",
    "\n",
    "Try to understand the quality of the data better. \n",
    "In total five types of data:\n",
    "\n",
    "    1. DMD patterns \n",
    "    2. Chromox real beam\n",
    "    3. YAG real beam\n",
    "    4. Chromox laser scan\n",
    "    5. Yag laser scan\n",
    "\n",
    "Beam data started from Wednesday (Chromox) (2025-11-19), then fiber state changed, then Friday and Saturday (Chromox, 2025-11-21 + 2025-11-22), Sunday (YAG, 2025-11-23), with laser scan also in Saturday and Sunday, plus DMD in the middle of sections (Many collected in 2025-11-20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())         \n",
    "\n",
    "from xflow import SqlProvider, pipe_each, TransformRegistry as T\n",
    "from xflow.utils import plot_image\n",
    "import xflow.extensions.physics\n",
    "from xflow.utils.io import scan_files\n",
    "from xflow.utils.sql import union_sqlite_db_tables, merge_sqlite_dbs\n",
    "\n",
    "from config_utils import load_config, detect_machine\n",
    "from utils import *\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "experiment_name = \"CAE_validate_clear\"  \n",
    "machine = detect_machine() \n",
    "\n",
    "config = load_config(\n",
    "    f\"{experiment_name}.yaml\",\n",
    "    machine=machine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87153b",
   "metadata": {},
   "source": [
    "# Scope 1 - DMD synthetic data and its corresponding Real data\n",
    "\n",
    "Create such ready to use dataset for training and evaluation (could reverse the training testing logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4860c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Merge entire CLEAR 2025 dataset in to a single database\n",
    "# ============================\n",
    "merged_path = \"C:\\\\Users\\\\qiyuanxu\\\\Desktop\\\\clear_2025_dataset.db\"\n",
    "db_paths = scan_files(\"C:\\\\Users\\\\qiyuanxu\\\\Documents\\\\DataHub\\\\datasets\", extensions=[\".db\"], return_type=\"str\")\n",
    "merge_sqlite_dbs(db_paths, output_path=merged_path, source_column=\"db_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Left join metadata into the big merged database to form a single complete table\n",
    "# ============================\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    d.*,\n",
    "    c.experiment_description,\n",
    "    c.image_source,\n",
    "    c.image_device,\n",
    "    c.fiber_config,\n",
    "    c.camera_config,\n",
    "    c.other_config\n",
    "FROM mmf_dataset_metadata AS d\n",
    "LEFT JOIN mmf_experiment_config AS c\n",
    "  ON c.id = d.config_id\n",
    " AND c.db_path = d.db_path;\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(str(merged_path)) as con:\n",
    "    tables_df = pd.read_sql_query(sql, con)\n",
    "\n",
    "# optional: drop duplicate column names (e.g. both tables have \"id\", \"db_path\")\n",
    "tables_df = tables_df.loc[:, ~tables_df.columns.duplicated()]\n",
    "print(tables_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee824de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# DMD hourly data collection statistics\n",
    "# ============================\n",
    "\n",
    "def hourly_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate rows by hour and return counts sorted chronologically.\"\"\"\n",
    "    return (\n",
    "        pd.to_datetime(df[\"create_time\"], errors=\"coerce\")\n",
    "          .dt.strftime(\"%Y-%m-%d %H\")\n",
    "          .dropna()\n",
    "          .value_counts()\n",
    "          .rename_axis(\"hour\")\n",
    "          .reset_index(name=\"count\")\n",
    "          .sort_values(\"hour\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "tables_df[\"other_config\"] = tables_df[\"other_config\"].map(\n",
    "    lambda x: x if isinstance(x, dict)\n",
    "    else json.loads(x) if isinstance(x, str) and x.strip().startswith(\"{\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "mask = tables_df[\"other_config\"].map(\n",
    "    lambda d: (\n",
    "        isinstance(d, dict)\n",
    "        and d.get(\"dmd_config\", {}).get(\"type\") != \"DummyDMD\"\n",
    "        and d.get(\"beam_settings\") is None\n",
    "    )\n",
    ")\n",
    "\n",
    "dmd_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(dmd_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Chromox hourly data collection statistics\n",
    "# ============================\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"Chromox\", na=False)\n",
    "chromox_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(chromox_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29297404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Yag hourly data collection statistics\n",
    "# ============================\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"YAG\", na=False)\n",
    "yag_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(yag_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61243b",
   "metadata": {},
   "source": [
    "# Merge/Compile target .db then based on the .db create the final dataset\n",
    "First select the data we want, compile to a single .db, create a column with abs path to samples, build datapipeline and go though this .db, keep the structure and save as a new ready to train folder\n",
    "1. DMD only validation\n",
    "2. If 1 works, DMD training + Chromox validation\n",
    "3. There's two setup of MMF, one before 2025-11-20, one after. We do one after first\n",
    "4. Figure out crop position\n",
    "\n",
    "In general the valid DMD + Chromox as one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_col_mmf_dataset_metadata = [\"image_id\", \"is_calibration\", \"batch\", \"purpose\", \"db_path\", \"image_path\", \"comments\", \"beam_settings\",\n",
    "                                 \"create_time\", \"is_saturated_ground_truth\", \"is_saturated_fiber_output\", \"config_id\"]\n",
    "core_col_mmf_experiment_config = [\"experiment_description\", \"image_source\", \"image_device\", \"camera_config\", \"other_config\"]\n",
    "\n",
    "\"\"\"\n",
    " '2025-11-19',                          # dataset 1\n",
    " '2025-11-20',\n",
    " '2025-11-20_DMD_for_Wednesday_1',      # dataset 1\n",
    " '2025-11-20_DMD_for_Wednesday_2',      # dataset 1\n",
    " '2025-11-21',\n",
    " '2025-11-21-morning',\n",
    " '2025-11-22',\n",
    " '2025-11-22-afternoon',\n",
    " '2025-11-22-morning'\n",
    "\"\"\"\n",
    "final_df = pd.concat([dmd_df, chromox_df], ignore_index=True, copy=False)\n",
    "final_df = final_df.loc[:, [c for c in core_col_mmf_dataset_metadata + core_col_mmf_experiment_config if c in final_df.columns]]\n",
    "base = final_df[\"db_path\"].fillna(\"\").str.rsplit(\"/\", n=2).str[0] + \"/\"\n",
    "final_df[\"image_path\"] = base + final_df[\"image_path\"].fillna(\"\").str.lstrip(\"/\")\n",
    "final_df[\"dataset\"] = final_df[\"db_path\"].astype(str).str.rsplit(\"/\", n=3).str[-3]\n",
    "final_df = final_df.drop(columns=[\"db_path\"])\n",
    "\n",
    "remove_datasets = ['2025-11-19','2025-11-20_DMD_for_Wednesday_1','2025-11-20_DMD_for_Wednesday_2']\n",
    "final_df = final_df[~final_df[\"dataset\"].isin(remove_datasets)].copy()\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cropping area\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "def add_image_features(df, path_col, extractors, in_place=True):\n",
    "    \"\"\"\n",
    "    extractors: {\"new_col_1\": fn1, \"new_col_2\": fn2, ...}\n",
    "      each fn takes left-half image as ndarray and returns a value (scalar / tuple / list)\n",
    "    \"\"\"\n",
    "    if not in_place:\n",
    "        df = df.copy()\n",
    "\n",
    "    results = {new_col: [] for new_col in extractors}\n",
    "\n",
    "    for p in tqdm(df[path_col].tolist(), desc=\"images\"):\n",
    "        if p is None or (isinstance(p, float) and np.isnan(p)) or str(p).strip() == \"\":\n",
    "            for new_col in results:\n",
    "                results[new_col].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                w, h = im.size\n",
    "                left_np = np.asarray(im.crop((0, 0, w // 2, h)))\n",
    "            for new_col, fn in extractors.items():\n",
    "                results[new_col].append(fn(left_np))\n",
    "        except Exception:\n",
    "            for new_col in results:\n",
    "                results[new_col].append(np.nan)\n",
    "\n",
    "    for new_col, vals in results.items():\n",
    "        df[new_col] = vals\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "extract_gaussian = partial(extract_beam_parameters, method=\"gaussian\")\n",
    "extract_moments = partial(extract_beam_parameters, method=\"moments\")\n",
    "\n",
    "final_df = add_image_features(\n",
    "    final_df,\n",
    "    path_col=\"image_path\",\n",
    "    extractors={\"gaussian_beam_params\": extract_gaussian, \"moments_beam_params\": extract_moments},\n",
    "    in_place=True,\n",
    ")\n",
    "\n",
    "db_out = \"C:/Users/qiyuanxu/Desktop/dataset_meta.db\"\n",
    "final_df = final_df.applymap(lambda x: str(x) if isinstance(x, (dict, list, tuple)) else x)\n",
    "with sqlite3.connect(db_out) as conn:\n",
    "    final_df.to_sql(\"mmf_dataset_metadata\", conn, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "vec_col = \"moments_beam_params\"\n",
    "ds_col  = \"dataset\"\n",
    "\n",
    "W, H = 1920, 1200          # image size\n",
    "R = 128                    # half-side length in pixels (distance from centroid to each edge)\n",
    "\n",
    "def parse_vec4(v):\n",
    "    if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "        return None\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        a = list(v)\n",
    "    else:\n",
    "        s = str(v).strip()\n",
    "        try:\n",
    "            a = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            s = s.strip(\"[]()\")\n",
    "            a = [t for t in s.replace(\",\", \" \").split() if t]\n",
    "    a = [float(x) for x in a]\n",
    "    return a if len(a) >= 2 else None\n",
    "\n",
    "def centroid_xy_norm(xy_series, W=None, H=None, R=None, verbose=True):\n",
    "    \"\"\"\n",
    "    xy_series: Series of list-like [x, y, ...] in normalized coords.\n",
    "    R: half-side length in px (crop size is 2R x 2R).\n",
    "    Returns:\n",
    "      (cx_norm, cy_norm), (cx_px, cy_px), ((tl_px), (br_px)), ((tl_int), (br_int_excl))\n",
    "    \"\"\"\n",
    "    arr = np.array([v[:2] for v in xy_series.dropna().to_list()], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return None\n",
    "\n",
    "    cx_norm, cy_norm = arr[:, 0].mean(), arr[:, 1].mean()\n",
    "    cx_px,  cy_px  = cx_norm * W, cy_norm * H\n",
    "\n",
    "    tl = (cx_px - R, cy_px - R)\n",
    "    br = (cx_px + R, cy_px + R)\n",
    "\n",
    "    # integer crop box with exact size (2R x 2R); br_int is EXCLUSIVE\n",
    "    R_int = int(R)\n",
    "    tl_int = (int(round(tl[0])), int(round(tl[1])))\n",
    "    br_int = (tl_int[0] + 2*R_int, tl_int[1] + 2*R_int)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"centroid (px): ({cx_px:.2f}, {cy_px:.2f})\")\n",
    "        print(f\"square TL, BR (px): ({tl[0]:.2f}, {tl[1]:.2f}), ({br[0]:.2f}, {br[1]:.2f})\")\n",
    "        print(f\"square TL, BR (int, size={2*R_int}x{2*R_int} px): {tl_int}, {br_int}\")\n",
    "\n",
    "    return (cx_norm, cy_norm), (cx_px, cy_px), (tl, br)\n",
    "\n",
    "\n",
    "xy = final_df[vec_col].apply(parse_vec4)\n",
    "mask = xy.notna() & final_df[ds_col].notna()\n",
    "\n",
    "# normalized points\n",
    "x_norm = xy[mask].apply(lambda a: a[0]).to_numpy()\n",
    "y_norm = xy[mask].apply(lambda a: a[1]).to_numpy()\n",
    "ds = pd.Categorical(final_df.loc[mask, ds_col].astype(str))\n",
    "\n",
    "# centroid in normalized coords\n",
    "c = centroid_xy_norm(xy[mask], W=W, H=H, R=R, verbose=True)\n",
    "(cx_norm, cy_norm), (cx, cy), (tl, br) = c\n",
    "\n",
    "# convert to pixel coords only for plotting\n",
    "x = x_norm * W\n",
    "y = y_norm * H\n",
    "cx = cx_norm * W\n",
    "cy = cy_norm * H\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "colors = [cmap(i % cmap.N) for i in ds.codes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15), dpi=120)\n",
    "ax.scatter(x, y, s=1, c=colors, alpha=0.5, edgecolors=\"none\")\n",
    "\n",
    "# legend (one color per dataset)\n",
    "for i, name in enumerate(ds.categories):\n",
    "    ax.scatter([], [], s=30, color=cmap(i % cmap.N), label=name)\n",
    "ax.legend(title=ds_col, loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "# centroid (star) + square of half-side length R\n",
    "ax.scatter([cx], [cy], marker=\"*\", s=20)\n",
    "ax.add_patch(Rectangle((cx - R, cy - R), 2*R, 2*R, fill=False, linewidth=2))\n",
    "\n",
    "ax.set_xlim(0, W)\n",
    "ax.set_ylim(0, H)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax.set_xlabel(\"x (px)\")\n",
    "ax.set_ylabel(\"y (px)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4142c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"C:\\\\Users\\\\qiyuanxu\\\\Desktop\\\\dataset_meta.db\"\n",
    "table = \"mmf_dataset_metadata\"\n",
    "\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    final_df = pd.read_sql_query(f'SELECT * FROM \"{table}\"', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Core dataset creation pipeline\n",
    "# ============================\n",
    "\n",
    "from xflow import pipe_each, consume\n",
    "from xflow import TransformRegistry as T\n",
    "from functools import partial\n",
    "\n",
    "params = {\n",
    "    \"crop_gt\": [[850, 478], [1106, 734]], \n",
    "    \"crop_fo\": [[360, 0], [1560, 1200]],\n",
    "    \"inp_size\": (256, 256),\n",
    "    \"out_size\": (256, 256)\n",
    "}\n",
    "\n",
    "out_dir = \"C:/Users/qiyuanxu/Desktop/CLEAR25_CHROMOX/dataset/\"\n",
    "image_paths = final_df[\"image_path\"].tolist()\n",
    "\n",
    "results = list(pipe_each(\n",
    "    [(p, p) for p in image_paths],\n",
    "    [T.get(\"torch_load_image\"), T.get(\"extract_stem\")],\n",
    "    [None, partial(T.get(\"add_parent_dir\"), parent_dir=out_dir)],\n",
    "    [None, partial(T.get(\"add_suffix\"), suffix=\".png\")],\n",
    "    # [None, T.get(\"debug_print\")],\n",
    "    [T.get(\"torch_to_grayscale\"), None],\n",
    "    [T.get(\"torch_remap_range\"), None],\n",
    "    [partial(T.get(\"torch_split_width\"), swap=True), None],\n",
    "    [\n",
    "        partial(T.get(\"torch_crop_area\"), points=params[\"crop_fo\"]),\n",
    "        partial(T.get(\"torch_crop_area\"), points=params[\"crop_gt\"]),\n",
    "        None,\n",
    "    ],\n",
    "    [\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"inp_size\"]),\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"out_size\"]),\n",
    "        None,\n",
    "    ],\n",
    "    partial(T.get(\"reorder\"), order=[1, 0, 2]),\n",
    "    [consume(2, lambda imgs: T.get(\"join_image\")(list(imgs), layout=(1, 2))), None],\n",
    "    T.get(\"save_image\"),  # Takes (image, full_path), saves, passes through\n",
    "    \n",
    "    progress=True,\n",
    "    desc=\"Processing images\",\n",
    "    skip_errors=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947368",
   "metadata": {},
   "source": [
    "# Scope 2 - Real data only with data augmentation pipeline (super position)\n",
    "Create such ready to use dataset for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da7aac",
   "metadata": {},
   "source": [
    "# Visualization (temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xflow.utils.visualization import stack_log_remap, stack_linear_clip\n",
    "stacked = stack_log_remap([x[1] for x in results])\n",
    "plot_image(stacked)\n",
    "\n",
    "stacked = stack_linear_clip([x[1] for x in results])\n",
    "plot_image(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "Point = Tuple[float, float]\n",
    "\n",
    "\n",
    "def _min_mass_segment(weights: np.ndarray, frac: float, eps: float = 1e-12) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Smallest contiguous index range [i, j) whose sum >= frac * total.\n",
    "    Returns (i, j) with j exclusive.\n",
    "    \"\"\"\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    n = w.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty weights.\")\n",
    "    total = float(w.sum())\n",
    "    if total <= eps:\n",
    "        # No mass -> return full range\n",
    "        return 0, n\n",
    "\n",
    "    target = frac * total\n",
    "    best_i, best_j = 0, n\n",
    "    best_len = n + 1\n",
    "\n",
    "    j = 0\n",
    "    s = 0.0\n",
    "    for i in range(n):\n",
    "        while j < n and s < target:\n",
    "            s += w[j]\n",
    "            j += 1\n",
    "        if s >= target:\n",
    "            if (j - i) < best_len:\n",
    "                best_len = j - i\n",
    "                best_i, best_j = i, j\n",
    "        s -= w[i]\n",
    "\n",
    "    return best_i, best_j\n",
    "\n",
    "\n",
    "def _clamp_interval(a: float, b: float, lo: float = 0.0, hi: float = 1.0) -> Tuple[float, float]:\n",
    "    \"\"\"Clamp [a,b] into [lo,hi] by shifting (keeps length if possible).\"\"\"\n",
    "    length = b - a\n",
    "    if length >= (hi - lo):\n",
    "        return lo, hi\n",
    "    if a < lo:\n",
    "        b = b + (lo - a)\n",
    "        a = lo\n",
    "    if b > hi:\n",
    "        a = a - (b - hi)\n",
    "        b = hi\n",
    "    a = max(lo, a)\n",
    "    b = min(hi, b)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def square_from_projections(\n",
    "    img: np.ndarray,\n",
    "    frac: float,\n",
    "    *,\n",
    "    make_square: bool = True,\n",
    "    channel_reduce: str = \"sum\",  # \"sum\" or \"mean\"\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[Point, Point]:\n",
    "    \"\"\"\n",
    "    1) Project image onto x and y by summing pixels.\n",
    "    2) Find minimal contiguous x-interval containing `frac` of x-projection mass,\n",
    "       and same for y.\n",
    "    3) Form rectangle. If make_square=True, expand to the smallest axis-aligned square\n",
    "       that contains that rectangle (centered), clamped to [0,1].\n",
    "\n",
    "    Returns (top_left, bottom_right) in normalized coords, with (0,0) at top-left.\n",
    "    \"\"\"\n",
    "    if frac > 1.0:\n",
    "        frac = frac / 100.0\n",
    "    if not (0.0 < frac <= 1.0):\n",
    "        raise ValueError(\"frac must be in (0,1] or (0,100].\")\n",
    "\n",
    "    a = np.asarray(img, dtype=float)\n",
    "    if a.ndim == 3:\n",
    "        if channel_reduce == \"mean\":\n",
    "            a = a.mean(axis=2)\n",
    "        elif channel_reduce == \"sum\":\n",
    "            a = a.sum(axis=2)\n",
    "        else:\n",
    "            raise ValueError(\"channel_reduce must be 'sum' or 'mean'\")\n",
    "    elif a.ndim != 2:\n",
    "        raise ValueError(\"img must be 2D or 3D array\")\n",
    "\n",
    "    # Ensure non-negative \"mass\"\n",
    "    a = np.clip(a, 0.0, None)\n",
    "\n",
    "    H, W = a.shape\n",
    "    if H == 0 or W == 0:\n",
    "        raise ValueError(\"img has zero size\")\n",
    "\n",
    "    proj_x = a.sum(axis=0)  # length W\n",
    "    proj_y = a.sum(axis=1)  # length H\n",
    "\n",
    "    ix0, ix1 = _min_mass_segment(proj_x, frac, eps=eps)  # [ix0, ix1)\n",
    "    iy0, iy1 = _min_mass_segment(proj_y, frac, eps=eps)  # [iy0, iy1)\n",
    "\n",
    "    # Convert pixel-edge indices to normalized [0,1]\n",
    "    x0, x1 = ix0 / W, ix1 / W\n",
    "    y0, y1 = iy0 / H, iy1 / H\n",
    "\n",
    "    if not make_square:\n",
    "        return (x0, y0), (x1, y1)\n",
    "\n",
    "    # Expand rectangle to a square (axis-aligned), centered on the rectangle\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    side = max(w, h)\n",
    "\n",
    "    cx = 0.5 * (x0 + x1)\n",
    "    cy = 0.5 * (y0 + y1)\n",
    "\n",
    "    sx0, sx1 = cx - 0.5 * side, cx + 0.5 * side\n",
    "    sy0, sy1 = cy - 0.5 * side, cy + 0.5 * side\n",
    "\n",
    "    sx0, sx1 = _clamp_interval(sx0, sx1, 0.0, 1.0)\n",
    "    sy0, sy1 = _clamp_interval(sy0, sy1, 0.0, 1.0)\n",
    "\n",
    "    return (sx0, sy0), (sx1, sy1)\n",
    "\n",
    "\n",
    "# Example:\n",
    "tl, br = square_from_projections(stacked, 0.96)\n",
    "highlighted = draw_red_square(stacked, tl, br, thickness=2)\n",
    "plot_image(highlighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array(results)\n",
    "hc, vc, hw, vw = a[:, 0], a[:, 1], a[:, 2], a[:, 3]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n",
    "\n",
    "ax[0].scatter(hc, vc, s=5, alpha=0.3)\n",
    "ax[0].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_centroid\", ylabel=\"v_centroid\", title=\"Centroids\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "\n",
    "ax[1].scatter(hw, vw, s=5, alpha=0.3)\n",
    "ax[1].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_width\", ylabel=\"v_width\", title=\"Widths\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d613e",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and read image sample paths\n",
    "# \"\"\"wednesday chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-19\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE \n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"Friday Chromox\"\"\"\n",
    "dirs = config[\"paths\"][\"chromox_2025-11-21\"] \n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM \n",
    "    mmf_dataset_metadata \n",
    "WHERE\n",
    "    batch IN (1, 2, 3)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"Saturday Chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-22-morning\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE\n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "db_path = f\"{dirs}/db/dataset_meta.db\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "image_paths = realbeam_provider()\n",
    "print(f\"Found {len(image_paths)} entries in the database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
