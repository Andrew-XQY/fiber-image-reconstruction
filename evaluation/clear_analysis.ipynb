{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801323da",
   "metadata": {},
   "source": [
    "# Dataset Statistics/exploration\n",
    "\n",
    "Try to understand the quality of the data better. \n",
    "In total five types of data:\n",
    "\n",
    "    1. DMD patterns \n",
    "    2. Chromox real beam\n",
    "    3. YAG real beam\n",
    "    4. Chromox laser scan\n",
    "    5. Yag laser scan\n",
    "\n",
    "Beam data started from Wednesday (Chromox) (2025-11-19), then Friday and Saturday (Chromox, 2025-11-21 + 2025-11-22), Sunday (YAG, 2025-11-23), with laser scan also in Saturday and Sunday, plus DMD in the middle of sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b4bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qiyuanxu\\Documents\\GitHub\\fiber-image-reconstruction-comparison\n",
      "[config_utils] Using machine profile: win-qiyuanxu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())         \n",
    "\n",
    "from xflow import SqlProvider, pipe_each, TransformRegistry as T\n",
    "from xflow.utils import plot_image\n",
    "import xflow.extensions.physics\n",
    "from xflow.utils.io import scan_files\n",
    "from xflow.utils.sql import union_sqlite_db_tables, merge_sqlite_dbs\n",
    "\n",
    "from config_utils import load_config, detect_machine\n",
    "from utils import *\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "experiment_name = \"CAE_validate_clear\"  \n",
    "machine = detect_machine() \n",
    "\n",
    "config = load_config(\n",
    "    f\"{experiment_name}.yaml\",\n",
    "    machine=machine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87153b",
   "metadata": {},
   "source": [
    "# Scope 1 - DMD synthetic data and its corresponding Real data\n",
    "\n",
    "Create such ready to use dataset for training and evaluation (could reverse the training testing logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4860c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qiyuanxu\\Documents\\GitHub\\XFlow\\src\\xflow\\utils\\sql.py:515: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\qiyuanxu\\\\Desktop\\\\clear_2025_dataset.db'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Merge entire CLEAR 2025 dataset in to a single database\n",
    "# ============================\n",
    "\n",
    "merged_path = \"C:\\\\Users\\\\qiyuanxu\\\\Desktop\\\\clear_2025_dataset.db\"\n",
    "db_paths = scan_files(\"C:\\\\Users\\\\qiyuanxu\\\\Documents\\\\DataHub\\\\datasets\", extensions=[\".db\"], return_type=\"str\")\n",
    "merge_sqlite_dbs(db_paths, output_path=merged_path, source_column=\"db_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9f5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93787, 31)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Left join metadata into the big merged database to form a single table\n",
    "# ============================\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    d.*,\n",
    "    c.experiment_description,\n",
    "    c.image_source,\n",
    "    c.image_device,\n",
    "    c.fiber_config,\n",
    "    c.camera_config,\n",
    "    c.other_config\n",
    "FROM mmf_dataset_metadata AS d\n",
    "LEFT JOIN mmf_experiment_config AS c\n",
    "  ON c.id = d.config_id\n",
    " AND c.db_path = d.db_path;\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(str(merged_path)) as con:\n",
    "    tables_df = pd.read_sql_query(sql, con)\n",
    "\n",
    "# optional: drop duplicate column names (e.g. both tables have \"id\", \"db_path\")\n",
    "tables_df = tables_df.loc[:, ~tables_df.columns.duplicated()]\n",
    "print(tables_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee824de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 10337\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a2d64cb1-71e1-464e-9ca9-58ddd1c36eec",
       "rows": [
        [
         "0",
         "2025-11-19 16",
         "225"
        ],
        [
         "1",
         "2025-11-19 17",
         "891"
        ],
        [
         "2",
         "2025-11-20 06",
         "25"
        ],
        [
         "3",
         "2025-11-20 07",
         "1096"
        ],
        [
         "4",
         "2025-11-20 08",
         "1436"
        ],
        [
         "5",
         "2025-11-20 10",
         "434"
        ],
        [
         "6",
         "2025-11-20 11",
         "2432"
        ],
        [
         "7",
         "2025-11-20 12",
         "173"
        ],
        [
         "8",
         "2025-11-20 19",
         "558"
        ],
        [
         "9",
         "2025-11-20 20",
         "394"
        ],
        [
         "10",
         "2025-11-20 21",
         "49"
        ],
        [
         "11",
         "2025-11-20 22",
         "920"
        ],
        [
         "12",
         "2025-11-22 20",
         "382"
        ],
        [
         "13",
         "2025-11-22 21",
         "1322"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 14
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-19 16</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-19 17</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-20 06</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-20 07</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-20 08</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-20 10</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-20 11</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-20 12</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-20 19</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-20 20</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-20 21</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-20 22</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-22 20</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-22 21</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             hour  count\n",
       "0   2025-11-19 16    225\n",
       "1   2025-11-19 17    891\n",
       "2   2025-11-20 06     25\n",
       "3   2025-11-20 07   1096\n",
       "4   2025-11-20 08   1436\n",
       "5   2025-11-20 10    434\n",
       "6   2025-11-20 11   2432\n",
       "7   2025-11-20 12    173\n",
       "8   2025-11-20 19    558\n",
       "9   2025-11-20 20    394\n",
       "10  2025-11-20 21     49\n",
       "11  2025-11-20 22    920\n",
       "12  2025-11-22 20    382\n",
       "13  2025-11-22 21   1322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# DMD hourly data collection statistics\n",
    "# ============================\n",
    "\n",
    "tables_df[\"other_config\"] = tables_df[\"other_config\"].map(\n",
    "    lambda x: x if isinstance(x, dict)\n",
    "    else json.loads(x) if isinstance(x, str) and x.strip().startswith(\"{\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "mask = tables_df[\"other_config\"].map(\n",
    "    lambda d: (\n",
    "        isinstance(d, dict)\n",
    "        and d.get(\"dmd_config\", {}).get(\"type\") != \"DummyDMD\"\n",
    "        and d.get(\"beam_settings\") is None\n",
    "    )\n",
    ")\n",
    "\n",
    "dmd_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = (\n",
    "    pd.to_datetime(dmd_df[\"create_time\"], errors=\"coerce\")\n",
    "      .dt.strftime(\"%Y-%m-%d %H\")\n",
    "      .dropna()\n",
    "      .value_counts()\n",
    "      .rename_axis(\"hour\")\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values(\"hour\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caca38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 49583\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "52292892-c018-482b-878f-1c959b3a64ef",
       "rows": [
        [
         "0",
         "2025-11-19 12",
         "326"
        ],
        [
         "1",
         "2025-11-19 13",
         "35"
        ],
        [
         "2",
         "2025-11-19 14",
         "785"
        ],
        [
         "3",
         "2025-11-19 15",
         "1842"
        ],
        [
         "4",
         "2025-11-21 08",
         "28"
        ],
        [
         "5",
         "2025-11-21 09",
         "4407"
        ],
        [
         "6",
         "2025-11-21 10",
         "4836"
        ],
        [
         "7",
         "2025-11-21 11",
         "247"
        ],
        [
         "8",
         "2025-11-21 22",
         "2913"
        ],
        [
         "9",
         "2025-11-21 23",
         "5047"
        ],
        [
         "10",
         "2025-11-22 00",
         "5784"
        ],
        [
         "11",
         "2025-11-22 01",
         "233"
        ],
        [
         "12",
         "2025-11-22 08",
         "582"
        ],
        [
         "13",
         "2025-11-22 09",
         "5674"
        ],
        [
         "14",
         "2025-11-22 10",
         "5294"
        ],
        [
         "15",
         "2025-11-22 11",
         "3794"
        ],
        [
         "16",
         "2025-11-22 12",
         "3030"
        ],
        [
         "17",
         "2025-11-22 13",
         "4473"
        ],
        [
         "18",
         "2025-11-22 14",
         "253"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 19
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-19 12</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-19 13</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-19 14</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-19 15</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-21 08</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-21 09</td>\n",
       "      <td>4407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-21 10</td>\n",
       "      <td>4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-21 11</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-21 22</td>\n",
       "      <td>2913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-21 23</td>\n",
       "      <td>5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-22 00</td>\n",
       "      <td>5784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-22 01</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-22 08</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-22 09</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-22 10</td>\n",
       "      <td>5294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-22 11</td>\n",
       "      <td>3794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-22 12</td>\n",
       "      <td>3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-22 13</td>\n",
       "      <td>4473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-22 14</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             hour  count\n",
       "0   2025-11-19 12    326\n",
       "1   2025-11-19 13     35\n",
       "2   2025-11-19 14    785\n",
       "3   2025-11-19 15   1842\n",
       "4   2025-11-21 08     28\n",
       "5   2025-11-21 09   4407\n",
       "6   2025-11-21 10   4836\n",
       "7   2025-11-21 11    247\n",
       "8   2025-11-21 22   2913\n",
       "9   2025-11-21 23   5047\n",
       "10  2025-11-22 00   5784\n",
       "11  2025-11-22 01    233\n",
       "12  2025-11-22 08    582\n",
       "13  2025-11-22 09   5674\n",
       "14  2025-11-22 10   5294\n",
       "15  2025-11-22 11   3794\n",
       "16  2025-11-22 12   3030\n",
       "17  2025-11-22 13   4473\n",
       "18  2025-11-22 14    253"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Chromox hourly data collection statistics\n",
    "# ============================\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"Chromox\", na=False)\n",
    "chromox_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = (\n",
    "    pd.to_datetime(chromox_df[\"create_time\"], errors=\"coerce\")\n",
    "      .dt.strftime(\"%Y-%m-%d %H\")\n",
    "      .dropna()\n",
    "      .value_counts()\n",
    "      .rename_axis(\"hour\")\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values(\"hour\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29297404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 15341\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cb74a6f9-25f7-4d5f-819a-425de6488baa",
       "rows": [
        [
         "0",
         "2025-11-23 09",
         "1954"
        ],
        [
         "1",
         "2025-11-23 10",
         "3648"
        ],
        [
         "2",
         "2025-11-23 11",
         "3406"
        ],
        [
         "3",
         "2025-11-23 12",
         "4694"
        ],
        [
         "4",
         "2025-11-23 13",
         "1639"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-23 09</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-23 10</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-23 11</td>\n",
       "      <td>3406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-23 12</td>\n",
       "      <td>4694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-23 13</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hour  count\n",
       "0  2025-11-23 09   1954\n",
       "1  2025-11-23 10   3648\n",
       "2  2025-11-23 11   3406\n",
       "3  2025-11-23 12   4694\n",
       "4  2025-11-23 13   1639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Yag hourly data collection statistics\n",
    "# ============================\n",
    "\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"YAG\", na=False)\n",
    "yag_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = (\n",
    "    pd.to_datetime(yag_df[\"create_time\"], errors=\"coerce\")\n",
    "      .dt.strftime(\"%Y-%m-%d %H\")\n",
    "      .dropna()\n",
    "      .value_counts()\n",
    "      .rename_axis(\"hour\")\n",
    "      .reset_index(name=\"count\")\n",
    "      .sort_values(\"hour\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ab52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and read image sample paths\n",
    "# \"\"\"wednesday chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-19\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE \n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"Friday Chromox\"\"\"\n",
    "dirs = config[\"paths\"][\"chromox_2025-11-21\"] \n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM \n",
    "    mmf_dataset_metadata \n",
    "WHERE\n",
    "    batch IN (1, 2, 3)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"Saturday Chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-22-morning\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE\n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "db_path = f\"{dirs}/db/dataset_meta.db\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "image_paths = realbeam_provider()\n",
    "print(f\"Found {len(image_paths)} entries in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "params = {\n",
    "    \"crop_gt\": [[74, 728], [1158, 488]],\n",
    "    \"crop_fo\": [[360, 0], [1560, 1200]],\n",
    "    \"inp_size\": (256, 256),\n",
    "    \"out_size\": (256, 256),\n",
    "}\n",
    "\n",
    "def default_meta(path):\n",
    "    \"\"\"Default metadata extractor - path, filename, extension.\"\"\"\n",
    "    p = Path(path)\n",
    "    return {\"path\": str(p), \"filename\": p.stem, \"extension\": p.suffix}\n",
    "    \n",
    "extract_moments = partial(extract_beam_parameters, method=\"moments\")\n",
    "\n",
    "\n",
    "# Define transform pipeline\n",
    "transforms = [\n",
    "    partial(T.get(\"add_parent_dir\"), parent_dir=dirs),\n",
    "    partial(T.get(\"torch_load_image_with_meta\"), meta_fn=default_meta),\n",
    "    # [None, T.get(\"debug_print\")], \n",
    "    [T.get(\"torch_to_tensor\"), None],  \n",
    "    [T.get(\"torch_to_grayscale\"), None],\n",
    "    [T.get(\"torch_remap_range\"), None],\n",
    "    [partial(T.get(\"torch_split_width\"), swap=True), None],    # (fiber_output, ground_truth)\n",
    "    # [\n",
    "    #     partial(T.get(\"torch_crop_area\"), points=params[\"crop_fo\"]),\n",
    "    #     partial(T.get(\"torch_crop_area\"), points=params[\"crop_gt\"]),\n",
    "    # ],\n",
    "    [\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"inp_size\"]),\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"out_size\"]),\n",
    "        None\n",
    "    ],\n",
    "    # [\n",
    "    #     T.get(\"discard\"),\n",
    "    #     partial(T.get(\"apply\"), fn=extract_moments)\n",
    "    # ],\n",
    "    # T.get(\"flatten_nested\"),\n",
    "    # T.get(\"collect\"),\n",
    "    T.get(\"raise_if_none\"),\n",
    "    T.get(\"join_image\"),\n",
    "    T.get(\"save_image_from_meta\"),\n",
    "]\n",
    "\n",
    "results = list(pipe_each(\n",
    "    image_paths,\n",
    "    *transforms,\n",
    "    progress=True,\n",
    "    desc=\"Processing images\",\n",
    "    skip_errors=True,\n",
    "))\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec212f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947368",
   "metadata": {},
   "source": [
    "# Scope 2 - Real data only with data augmentation pipeline (super position)\n",
    "Create such ready to use dataset for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da7aac",
   "metadata": {},
   "source": [
    "# Visualization (temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xflow.utils.visualization import stack_log_remap, stack_linear_clip\n",
    "stacked = stack_log_remap([x[1] for x in results])\n",
    "plot_image(stacked)\n",
    "\n",
    "stacked = stack_linear_clip([x[1] for x in results])\n",
    "plot_image(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "Point = Tuple[float, float]\n",
    "\n",
    "\n",
    "def _min_mass_segment(weights: np.ndarray, frac: float, eps: float = 1e-12) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Smallest contiguous index range [i, j) whose sum >= frac * total.\n",
    "    Returns (i, j) with j exclusive.\n",
    "    \"\"\"\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    n = w.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty weights.\")\n",
    "    total = float(w.sum())\n",
    "    if total <= eps:\n",
    "        # No mass -> return full range\n",
    "        return 0, n\n",
    "\n",
    "    target = frac * total\n",
    "    best_i, best_j = 0, n\n",
    "    best_len = n + 1\n",
    "\n",
    "    j = 0\n",
    "    s = 0.0\n",
    "    for i in range(n):\n",
    "        while j < n and s < target:\n",
    "            s += w[j]\n",
    "            j += 1\n",
    "        if s >= target:\n",
    "            if (j - i) < best_len:\n",
    "                best_len = j - i\n",
    "                best_i, best_j = i, j\n",
    "        s -= w[i]\n",
    "\n",
    "    return best_i, best_j\n",
    "\n",
    "\n",
    "def _clamp_interval(a: float, b: float, lo: float = 0.0, hi: float = 1.0) -> Tuple[float, float]:\n",
    "    \"\"\"Clamp [a,b] into [lo,hi] by shifting (keeps length if possible).\"\"\"\n",
    "    length = b - a\n",
    "    if length >= (hi - lo):\n",
    "        return lo, hi\n",
    "    if a < lo:\n",
    "        b = b + (lo - a)\n",
    "        a = lo\n",
    "    if b > hi:\n",
    "        a = a - (b - hi)\n",
    "        b = hi\n",
    "    a = max(lo, a)\n",
    "    b = min(hi, b)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def square_from_projections(\n",
    "    img: np.ndarray,\n",
    "    frac: float,\n",
    "    *,\n",
    "    make_square: bool = True,\n",
    "    channel_reduce: str = \"sum\",  # \"sum\" or \"mean\"\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[Point, Point]:\n",
    "    \"\"\"\n",
    "    1) Project image onto x and y by summing pixels.\n",
    "    2) Find minimal contiguous x-interval containing `frac` of x-projection mass,\n",
    "       and same for y.\n",
    "    3) Form rectangle. If make_square=True, expand to the smallest axis-aligned square\n",
    "       that contains that rectangle (centered), clamped to [0,1].\n",
    "\n",
    "    Returns (top_left, bottom_right) in normalized coords, with (0,0) at top-left.\n",
    "    \"\"\"\n",
    "    if frac > 1.0:\n",
    "        frac = frac / 100.0\n",
    "    if not (0.0 < frac <= 1.0):\n",
    "        raise ValueError(\"frac must be in (0,1] or (0,100].\")\n",
    "\n",
    "    a = np.asarray(img, dtype=float)\n",
    "    if a.ndim == 3:\n",
    "        if channel_reduce == \"mean\":\n",
    "            a = a.mean(axis=2)\n",
    "        elif channel_reduce == \"sum\":\n",
    "            a = a.sum(axis=2)\n",
    "        else:\n",
    "            raise ValueError(\"channel_reduce must be 'sum' or 'mean'\")\n",
    "    elif a.ndim != 2:\n",
    "        raise ValueError(\"img must be 2D or 3D array\")\n",
    "\n",
    "    # Ensure non-negative \"mass\"\n",
    "    a = np.clip(a, 0.0, None)\n",
    "\n",
    "    H, W = a.shape\n",
    "    if H == 0 or W == 0:\n",
    "        raise ValueError(\"img has zero size\")\n",
    "\n",
    "    proj_x = a.sum(axis=0)  # length W\n",
    "    proj_y = a.sum(axis=1)  # length H\n",
    "\n",
    "    ix0, ix1 = _min_mass_segment(proj_x, frac, eps=eps)  # [ix0, ix1)\n",
    "    iy0, iy1 = _min_mass_segment(proj_y, frac, eps=eps)  # [iy0, iy1)\n",
    "\n",
    "    # Convert pixel-edge indices to normalized [0,1]\n",
    "    x0, x1 = ix0 / W, ix1 / W\n",
    "    y0, y1 = iy0 / H, iy1 / H\n",
    "\n",
    "    if not make_square:\n",
    "        return (x0, y0), (x1, y1)\n",
    "\n",
    "    # Expand rectangle to a square (axis-aligned), centered on the rectangle\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    side = max(w, h)\n",
    "\n",
    "    cx = 0.5 * (x0 + x1)\n",
    "    cy = 0.5 * (y0 + y1)\n",
    "\n",
    "    sx0, sx1 = cx - 0.5 * side, cx + 0.5 * side\n",
    "    sy0, sy1 = cy - 0.5 * side, cy + 0.5 * side\n",
    "\n",
    "    sx0, sx1 = _clamp_interval(sx0, sx1, 0.0, 1.0)\n",
    "    sy0, sy1 = _clamp_interval(sy0, sy1, 0.0, 1.0)\n",
    "\n",
    "    return (sx0, sy0), (sx1, sy1)\n",
    "\n",
    "\n",
    "# Example:\n",
    "tl, br = square_from_projections(stacked, 0.96)\n",
    "highlighted = draw_red_square(stacked, tl, br, thickness=2)\n",
    "plot_image(highlighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array(results)\n",
    "hc, vc, hw, vw = a[:, 0], a[:, 1], a[:, 2], a[:, 3]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n",
    "\n",
    "ax[0].scatter(hc, vc, s=5, alpha=0.3)\n",
    "ax[0].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_centroid\", ylabel=\"v_centroid\", title=\"Centroids\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "\n",
    "ax[1].scatter(hw, vw, s=5, alpha=0.3)\n",
    "ax[1].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_width\", ylabel=\"v_width\", title=\"Widths\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
