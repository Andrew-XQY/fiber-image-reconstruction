{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801323da",
   "metadata": {},
   "source": [
    "# Dataset Statistics/exploration\n",
    "\n",
    "Try to understand the quality of the data better. \n",
    "In total five types of data:\n",
    "\n",
    "    1. DMD patterns \n",
    "    2. Chromox real beam\n",
    "    3. YAG real beam\n",
    "    4. Chromox laser scan\n",
    "    5. Yag laser scan\n",
    "\n",
    "Beam data started from Wednesday (Chromox) (2025-11-19), then fiber state changed, then Friday and Saturday (Chromox, 2025-11-21 + 2025-11-22), Sunday (YAG, 2025-11-23), with laser scan also in Saturday and Sunday, plus DMD in the middle of sections (Many collected in 2025-11-20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b4bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrewxu/Documents/GitHub/fiber-image-reconstruction\n",
      "[config_utils] Using machine profile: mac-andrewxu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())         \n",
    "\n",
    "from xflow import SqlProvider, flow, TransformRegistry as T\n",
    "from xflow.utils import plot_image\n",
    "import xflow.extensions.physics\n",
    "from xflow.utils.io import scan_files\n",
    "from xflow.utils.sql import union_sqlite_db_tables, merge_sqlite_dbs\n",
    "\n",
    "from config_utils import load_config, detect_machine\n",
    "from utils import *\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "experiment_name = \"CAE_validate_clear\"  \n",
    "machine = detect_machine() \n",
    "\n",
    "config = load_config(\n",
    "    f\"{experiment_name}.yaml\",\n",
    "    machine=machine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87153b",
   "metadata": {},
   "source": [
    "# Scope 1 - DMD synthetic data and its corresponding Real data\n",
    "\n",
    "Create such ready to use dataset for training and evaluation (could reverse the training testing logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b95c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all directories\n",
    "\n",
    "# windows\n",
    "# dirs = {\n",
    "#     \"merged_db_path\": \"C:/Users/qiyuanxu/Desktop/clear_2025_dataset.db\",\n",
    "#     \"raw_db_dir\": \"C:/Users/qiyuanxu/Documents/DataHub/datasets\",\n",
    "#     \"final_merged_db_path\": \"C:/Users/qiyuanxu/Desktop/dataset_meta.db\",\n",
    "#     \"output_dataset_dir\": \"C:/Users/qiyuanxu/Desktop/CLEAR25_DMD/dataset/\",\n",
    "#     \"output_db_dir\": \"C:/Users/qiyuanxu/Desktop/CLEAR25_DMD/db/dataset_meta.db\"\n",
    "# }\n",
    "\n",
    "# Mac\n",
    "dirs = {\n",
    "    \"merged_db_path\": \"/Users/andrewxu/Desktop/clear_2025_dataset.db\",\n",
    "    \"raw_db_dir\": \"/Users/andrewxu/Documents/DataHub/datasets\",\n",
    "    \"final_merged_db_path\": \"/Users/andrewxu/Desktop/dataset_meta.db\",\n",
    "    \"output_dataset_dir\": \"/Users/andrewxu/Desktop/CLEAR25_DMD/dataset/\",\n",
    "    \"output_db_dir\": \"/Users/andrewxu/Desktop/CLEAR25_DMD/db/dataset_meta.db\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4860c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewxu/Documents/GitHub/XFlow/src/xflow/utils/sql.py:511: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/andrewxu/Desktop/clear_2025_dataset.db'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Merge entire CLEAR 2025 dataset in to a single database\n",
    "# ============================\n",
    "db_paths = scan_files(dirs[\"raw_db_dir\"], extensions=[\".db\"], return_type=\"str\")\n",
    "merge_sqlite_dbs(db_paths, output_path=dirs[\"merged_db_path\"], source_column=\"db_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9f5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91841, 31)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Left join metadata into the big merged database to form a single complete table\n",
    "# ============================\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    d.*,\n",
    "    c.experiment_description,\n",
    "    c.image_source,\n",
    "    c.image_device,\n",
    "    c.fiber_config,\n",
    "    c.camera_config,\n",
    "    c.other_config\n",
    "FROM mmf_dataset_metadata AS d\n",
    "LEFT JOIN mmf_experiment_config AS c\n",
    "  ON c.id = d.config_id\n",
    " AND c.db_path = d.db_path;\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(str(dirs[\"merged_db_path\"])) as con:\n",
    "    tables_df = pd.read_sql_query(sql, con)\n",
    "\n",
    "# optional: drop duplicate column names (e.g. both tables have \"id\", \"db_path\")\n",
    "tables_df = tables_df.loc[:, ~tables_df.columns.duplicated()]\n",
    "print(tables_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee824de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 8416\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "db091639-cb8e-4113-be5a-677a40cf2c29",
       "rows": [
        [
         "0",
         "2025-11-19 16",
         "225"
        ],
        [
         "1",
         "2025-11-19 17",
         "891"
        ],
        [
         "2",
         "2025-11-20 06",
         "25"
        ],
        [
         "3",
         "2025-11-20 07",
         "1096"
        ],
        [
         "4",
         "2025-11-20 08",
         "1436"
        ],
        [
         "5",
         "2025-11-20 10",
         "434"
        ],
        [
         "6",
         "2025-11-20 11",
         "2432"
        ],
        [
         "7",
         "2025-11-20 12",
         "173"
        ],
        [
         "8",
         "2025-11-22 20",
         "382"
        ],
        [
         "9",
         "2025-11-22 21",
         "1322"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-19 16</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-19 17</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-20 06</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-20 07</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-20 08</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-20 10</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-20 11</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-20 12</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-22 20</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-22 21</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hour  count\n",
       "0  2025-11-19 16    225\n",
       "1  2025-11-19 17    891\n",
       "2  2025-11-20 06     25\n",
       "3  2025-11-20 07   1096\n",
       "4  2025-11-20 08   1436\n",
       "5  2025-11-20 10    434\n",
       "6  2025-11-20 11   2432\n",
       "7  2025-11-20 12    173\n",
       "8  2025-11-22 20    382\n",
       "9  2025-11-22 21   1322"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# DMD hourly data collection statistics\n",
    "# ============================\n",
    "\n",
    "def hourly_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate rows by hour and return counts sorted chronologically.\"\"\"\n",
    "    return (\n",
    "        pd.to_datetime(df[\"create_time\"], errors=\"coerce\")\n",
    "          .dt.strftime(\"%Y-%m-%d %H\")\n",
    "          .dropna()\n",
    "          .value_counts()\n",
    "          .rename_axis(\"hour\")\n",
    "          .reset_index(name=\"count\")\n",
    "          .sort_values(\"hour\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "tables_df[\"other_config\"] = tables_df[\"other_config\"].map(\n",
    "    lambda x: x if isinstance(x, dict)\n",
    "    else json.loads(x) if isinstance(x, str) and x.strip().startswith(\"{\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "mask = tables_df[\"other_config\"].map(\n",
    "    lambda d: (\n",
    "        isinstance(d, dict)\n",
    "        and d.get(\"dmd_config\", {}).get(\"type\") != \"DummyDMD\"\n",
    "        and d.get(\"beam_settings\") is None\n",
    "    )\n",
    ")\n",
    "\n",
    "dmd_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(dmd_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caca38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 49583\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "2bbc913c-245c-4e23-ae4a-f9359d635b4b",
       "rows": [
        [
         "0",
         "2025-11-19 12",
         "326"
        ],
        [
         "1",
         "2025-11-19 13",
         "35"
        ],
        [
         "2",
         "2025-11-19 14",
         "785"
        ],
        [
         "3",
         "2025-11-19 15",
         "1842"
        ],
        [
         "4",
         "2025-11-21 08",
         "28"
        ],
        [
         "5",
         "2025-11-21 09",
         "4407"
        ],
        [
         "6",
         "2025-11-21 10",
         "4836"
        ],
        [
         "7",
         "2025-11-21 11",
         "247"
        ],
        [
         "8",
         "2025-11-21 22",
         "2913"
        ],
        [
         "9",
         "2025-11-21 23",
         "5047"
        ],
        [
         "10",
         "2025-11-22 00",
         "5784"
        ],
        [
         "11",
         "2025-11-22 01",
         "233"
        ],
        [
         "12",
         "2025-11-22 08",
         "582"
        ],
        [
         "13",
         "2025-11-22 09",
         "5674"
        ],
        [
         "14",
         "2025-11-22 10",
         "5294"
        ],
        [
         "15",
         "2025-11-22 11",
         "3794"
        ],
        [
         "16",
         "2025-11-22 12",
         "3030"
        ],
        [
         "17",
         "2025-11-22 13",
         "4473"
        ],
        [
         "18",
         "2025-11-22 14",
         "253"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 19
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-19 12</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-19 13</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-19 14</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-19 15</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-21 08</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-21 09</td>\n",
       "      <td>4407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-21 10</td>\n",
       "      <td>4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-21 11</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-21 22</td>\n",
       "      <td>2913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-21 23</td>\n",
       "      <td>5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-22 00</td>\n",
       "      <td>5784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-22 01</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-22 08</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-22 09</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-22 10</td>\n",
       "      <td>5294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-22 11</td>\n",
       "      <td>3794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-22 12</td>\n",
       "      <td>3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-22 13</td>\n",
       "      <td>4473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-22 14</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             hour  count\n",
       "0   2025-11-19 12    326\n",
       "1   2025-11-19 13     35\n",
       "2   2025-11-19 14    785\n",
       "3   2025-11-19 15   1842\n",
       "4   2025-11-21 08     28\n",
       "5   2025-11-21 09   4407\n",
       "6   2025-11-21 10   4836\n",
       "7   2025-11-21 11    247\n",
       "8   2025-11-21 22   2913\n",
       "9   2025-11-21 23   5047\n",
       "10  2025-11-22 00   5784\n",
       "11  2025-11-22 01    233\n",
       "12  2025-11-22 08    582\n",
       "13  2025-11-22 09   5674\n",
       "14  2025-11-22 10   5294\n",
       "15  2025-11-22 11   3794\n",
       "16  2025-11-22 12   3030\n",
       "17  2025-11-22 13   4473\n",
       "18  2025-11-22 14    253"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Chromox hourly data collection statistics\n",
    "# ============================\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"Chromox\", na=False)\n",
    "chromox_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(chromox_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29297404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows kept: 15341\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "513b1268-e211-4de1-916a-dc673660d089",
       "rows": [
        [
         "0",
         "2025-11-23 09",
         "1954"
        ],
        [
         "1",
         "2025-11-23 10",
         "3648"
        ],
        [
         "2",
         "2025-11-23 11",
         "3406"
        ],
        [
         "3",
         "2025-11-23 12",
         "4694"
        ],
        [
         "4",
         "2025-11-23 13",
         "1639"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-23 09</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-23 10</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-23 11</td>\n",
       "      <td>3406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-23 12</td>\n",
       "      <td>4694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-23 13</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hour  count\n",
       "0  2025-11-23 09   1954\n",
       "1  2025-11-23 10   3648\n",
       "2  2025-11-23 11   3406\n",
       "3  2025-11-23 12   4694\n",
       "4  2025-11-23 13   1639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Yag hourly data collection statistics\n",
    "# ============================\n",
    "mask = tables_df[\"beam_settings\"].notna() & tables_df[\"image_device\"].astype(str).str.contains(\"YAG\", na=False)\n",
    "yag_df = tables_df.loc[mask].copy()\n",
    "print(\"Total rows kept:\", int(mask.sum()))\n",
    "\n",
    "out = hourly_counts(yag_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61243b",
   "metadata": {},
   "source": [
    "# Merge/Compile target .db then based on the .db create the final dataset\n",
    "First select the data we want, compile to a single .db, create a column with abs path to samples, build datapipeline and go though this .db, keep the structure and save as a new ready to train folder\n",
    "1. DMD only validation\n",
    "2. If 1 works, DMD training + Chromox validation\n",
    "3. There's two setup of MMF, one before 2025-11-20, one after. We do one after first\n",
    "4. Figure out crop position\n",
    "\n",
    "In general the valid DMD + Chromox as one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_col_mmf_dataset_metadata = [\"image_id\", \"is_calibration\", \"batch\", \"purpose\", \"db_path\", \"image_path\", \"comments\", \"beam_settings\",\n",
    "                                 \"create_time\", \"is_saturated_ground_truth\", \"is_saturated_fiber_output\", \"config_id\"]\n",
    "core_col_mmf_experiment_config = [\"experiment_description\", \"image_source\", \"image_device\", \"camera_config\", \"other_config\"]\n",
    "\n",
    "\"\"\"\n",
    " '2025-11-19',                          # dataset 1\n",
    " '2025-11-20',\n",
    " '2025-11-20_DMD_for_Wednesday_1',      # dataset 1\n",
    " '2025-11-20_DMD_for_Wednesday_2',      # dataset 1\n",
    " '2025-11-21',\n",
    " '2025-11-21-morning',\n",
    " '2025-11-22',\n",
    " '2025-11-22-afternoon',\n",
    " '2025-11-22-morning'\n",
    "\"\"\"\n",
    "\n",
    "df_to_merge = [dmd_df] # dmd_df, chromox_df, yag_df\n",
    "\n",
    "final_df = pd.concat(df_to_merge, ignore_index=True, copy=False)\n",
    "final_df = final_df.loc[:, [c for c in core_col_mmf_dataset_metadata + core_col_mmf_experiment_config if c in final_df.columns]]\n",
    "base = final_df[\"db_path\"].fillna(\"\").str.rsplit(\"/\", n=2).str[0] + \"/\"\n",
    "final_df[\"image_path\"] = base + final_df[\"image_path\"].fillna(\"\").str.lstrip(\"/\")\n",
    "final_df[\"dataset\"] = final_df[\"db_path\"].astype(str).str.rsplit(\"/\", n=3).str[-3]\n",
    "final_df = final_df.drop(columns=[\"db_path\"])\n",
    "\n",
    "remove_datasets = ['2025-11-19','2025-11-20_DMD_for_Wednesday_1','2025-11-20_DMD_for_Wednesday_2']\n",
    "final_df = final_df[~final_df[\"dataset\"].isin(remove_datasets)].copy()\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Extract data sample beam parameters\n",
    "# ============================\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from xflow.extensions.physics.beam import extract_beam_parameters\n",
    "\n",
    "def add_image_features(df, path_col, extractors, in_place=True):\n",
    "    \"\"\"\n",
    "    extractors: {\"new_col_1\": fn1, \"new_col_2\": fn2, ...}\n",
    "      each fn takes left-half image as ndarray and returns a value (scalar / tuple / list)\n",
    "    \"\"\"\n",
    "    if not in_place:\n",
    "        df = df.copy()\n",
    "\n",
    "    results = {new_col: [] for new_col in extractors}\n",
    "\n",
    "    for p in tqdm(df[path_col].tolist(), desc=\"images\"):\n",
    "        if p is None or (isinstance(p, float) and np.isnan(p)) or str(p).strip() == \"\":\n",
    "            for new_col in results:\n",
    "                results[new_col].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                w, h = im.size\n",
    "                left_np = np.asarray(im.crop((0, 0, w // 2, h)))\n",
    "            for new_col, fn in extractors.items():\n",
    "                results[new_col].append(fn(left_np))\n",
    "        except Exception:\n",
    "            for new_col in results:\n",
    "                results[new_col].append(np.nan)\n",
    "\n",
    "    for new_col, vals in results.items():\n",
    "        df[new_col] = vals\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "extract_gaussian = partial(extract_beam_parameters, method=\"gaussian\")\n",
    "extract_moments = partial(extract_beam_parameters, method=\"moments\")\n",
    "\n",
    "final_df = add_image_features(\n",
    "    final_df,\n",
    "    path_col=\"image_path\",\n",
    "    extractors={\"gaussian_beam_params\": extract_gaussian, \"moments_beam_params\": extract_moments},\n",
    "    in_place=True,\n",
    ")\n",
    "\n",
    "final_df = final_df.applymap(lambda x: str(x) if isinstance(x, (dict, list, tuple)) else x)\n",
    "with sqlite3.connect(dirs[\"final_merged_db_path\"]) as conn:\n",
    "    final_df.to_sql(\"mmf_dataset_metadata\", conn, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Plot beam centroids and crop square, Define the cropping area\n",
    "# ============================\n",
    "\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "vec_col = \"moments_beam_params\"\n",
    "ds_col  = \"dataset\"\n",
    "\n",
    "W, H = 1920, 1200          # image size\n",
    "R = 230                    # half-side length in pixels (distance from centroid to each edge)\n",
    "\n",
    "def parse_vec4(v):\n",
    "    if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "        return None\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        a = list(v)\n",
    "    else:\n",
    "        s = str(v).strip()\n",
    "        try:\n",
    "            a = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            s = s.strip(\"[]()\")\n",
    "            a = [t for t in s.replace(\",\", \" \").split() if t]\n",
    "    a = [float(x) for x in a]\n",
    "    return a if len(a) >= 2 else None\n",
    "\n",
    "def centroid_xy_norm(xy_series, W=None, H=None, R=None, verbose=True):\n",
    "    \"\"\"\n",
    "    xy_series: Series of list-like [x, y, ...] in normalized coords.\n",
    "    R: half-side length in px (crop size is 2R x 2R).\n",
    "    Returns:\n",
    "      (cx_norm, cy_norm), (cx_px, cy_px), ((tl_px), (br_px)), ((tl_int), (br_int_excl))\n",
    "    \"\"\"\n",
    "    arr = np.array([v[:2] for v in xy_series.dropna().to_list()], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return None\n",
    "\n",
    "    cx_norm, cy_norm = arr[:, 0].mean(), arr[:, 1].mean()\n",
    "    cx_px,  cy_px  = cx_norm * W, cy_norm * H\n",
    "\n",
    "    tl = (cx_px - R, cy_px - R)\n",
    "    br = (cx_px + R, cy_px + R)\n",
    "\n",
    "    # integer crop box with exact size (2R x 2R); br_int is EXCLUSIVE\n",
    "    R_int = int(R)\n",
    "    tl_int = (int(round(tl[0])), int(round(tl[1])))\n",
    "    br_int = (tl_int[0] + 2*R_int, tl_int[1] + 2*R_int)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"centroid (px): ({cx_px:.2f}, {cy_px:.2f})\")\n",
    "        print(f\"square TL, BR (px): ({tl[0]:.2f}, {tl[1]:.2f}), ({br[0]:.2f}, {br[1]:.2f})\")\n",
    "        print(f\"square TL, BR (int, size={2*R_int}x{2*R_int} px): {tl_int}, {br_int}\")\n",
    "\n",
    "    return (cx_norm, cy_norm), (cx_px, cy_px), (tl, br)\n",
    "\n",
    "\n",
    "xy = final_df[vec_col].apply(parse_vec4)\n",
    "mask = xy.notna() & final_df[ds_col].notna()\n",
    "\n",
    "# normalized points\n",
    "x_norm = xy[mask].apply(lambda a: a[0]).to_numpy()\n",
    "y_norm = xy[mask].apply(lambda a: a[1]).to_numpy()\n",
    "ds = pd.Categorical(final_df.loc[mask, ds_col].astype(str))\n",
    "\n",
    "# centroid in normalized coords\n",
    "c = centroid_xy_norm(xy[mask], W=W, H=H, R=R, verbose=True)\n",
    "(cx_norm, cy_norm), (cx, cy), (tl, br) = c\n",
    "\n",
    "# convert to pixel coords only for plotting\n",
    "x = x_norm * W\n",
    "y = y_norm * H\n",
    "cx = cx_norm * W\n",
    "cy = cy_norm * H\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "colors = [cmap(i % cmap.N) for i in ds.codes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15), dpi=120)\n",
    "ax.scatter(x, y, s=1, c=colors, alpha=0.5, edgecolors=\"none\")\n",
    "\n",
    "# legend (one color per dataset)\n",
    "for i, name in enumerate(ds.categories):\n",
    "    ax.scatter([], [], s=30, color=cmap(i % cmap.N), label=name)\n",
    "ax.legend(title=ds_col, loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "\n",
    "# centroid (star) + square of half-side length R\n",
    "ax.scatter([cx], [cy], marker=\"*\", s=20)\n",
    "ax.add_patch(Rectangle((cx - R, cy - R), 2*R, 2*R, fill=False, linewidth=2))\n",
    "\n",
    "ax.set_xlim(0, W)\n",
    "ax.set_ylim(0, H)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax.set_xlabel(\"x (px)\")\n",
    "ax.set_ylabel(\"y (px)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4142c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to rerun, just read the final merged database and continue analysis\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "table = \"mmf_dataset_metadata\"\n",
    "\n",
    "with sqlite3.connect(dirs[\"final_merged_db_path\"]) as conn:\n",
    "    final_df = pd.read_sql_query(f'SELECT * FROM \"{table}\"', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Core dataset creation pipeline\n",
    "# ============================\n",
    "\n",
    "from xflow import flow, consume\n",
    "from xflow import TransformRegistry as T\n",
    "from functools import partial\n",
    "\n",
    "params = {\n",
    "    \"crop_gt\": [(742, 341), (1202, 801)],    # [[850, 478], [1106, 734]],\n",
    "    \"crop_fo\": [[360, 0], [1560, 1200]], \n",
    "    \"inp_size\": (256, 256),\n",
    "    \"out_size\": (256, 256)\n",
    "}\n",
    "\n",
    "image_paths = final_df[\"image_path\"].tolist()\n",
    "\n",
    "results = list(flow(\n",
    "    [(p, p) for p in image_paths],\n",
    "    [T.get(\"torch_load_image\"), T.get(\"extract_stem\")],\n",
    "    [None, partial(T.get(\"add_parent_dir\"), parent_dir=dirs[\"output_dataset_dir\"])], # CLEAR25_CHROMOX\n",
    "    [None, partial(T.get(\"add_suffix\"), suffix=\".png\")],\n",
    "    # [None, T.get(\"debug_print\")],\n",
    "    [T.get(\"torch_to_grayscale\"), None],\n",
    "    [T.get(\"torch_remap_range\"), None],\n",
    "    [partial(T.get(\"torch_split_width\"), swap=True), None],\n",
    "    [\n",
    "        partial(T.get(\"torch_crop_area\"), points=params[\"crop_fo\"]),\n",
    "        partial(T.get(\"torch_crop_area\"), points=params[\"crop_gt\"]),\n",
    "        None,\n",
    "    ],\n",
    "    [\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"inp_size\"]),\n",
    "        partial(T.get(\"torch_resize\"), size=params[\"out_size\"]),\n",
    "        None,\n",
    "    ],\n",
    "    partial(T.get(\"reorder\"), order=[1, 0, 2]),\n",
    "    [consume(2, lambda imgs: T.get(\"join_image\")(list(imgs), layout=(1, 2))), None],\n",
    "    T.get(\"save_image\"),  # Takes (image, full_path), saves, passes through\n",
    "    \n",
    "    progress=True,\n",
    "    desc=\"Processing images\",\n",
    "    skip_errors=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save the metadata dataframe as .db, then it is ready for training on HPC/GPU\n",
    "db_path = \"C:\\\\Users\\\\qiyuanxu\\\\Desktop\\\\CLEAR25_DMD\\\\db\\\\dataset_meta.db\"\n",
    "\n",
    "df_save = final_df.copy()\n",
    "s = df_save[\"image_path\"]\n",
    "df_save[\"image_path\"] = s.where(s.isna(), s.astype(str).str.replace(r\"^.*[\\\\/]\", \"\", regex=True))\n",
    "\n",
    "with sqlite3.connect(dirs[\"output_db_dir\"]) as conn:\n",
    "    df_save.to_sql(\"mmf_dataset_metadata\", conn, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13092dac",
   "metadata": {},
   "source": [
    "## quick test of the created basis pipeline before HPC\n",
    "Once the processed dataset is created, could directly run below (and the first config cell) cells without run anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xflow.extensions.physics.pipeline import CachedBasisPipeline, LinearCombinator, uniform_k, PatternDecompositionCombinator\n",
    "from xflow.data import build_transforms_from_config, PyTorchPipeline\n",
    "from xflow.extensions.physics import pattern_gen\n",
    "\n",
    "# training set (basis -> generative pipeline)\n",
    "query = \"\"\" \n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE purpose = 'intensity_position' and batch IN (9)\n",
    "\"\"\"\n",
    "train_provider = SqlProvider(\n",
    "    sources={\"connection\": dirs[\"output_db_dir\"], \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "# evaluation set (validation + test, using real beam pattern loaded on the DMD)\n",
    "query = \"\"\" \n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE experiment_description = 'local real beam image for evaluation'\n",
    "--WHERE comments = 'test dmd'\n",
    "\"\"\"\n",
    "eval_provider = SqlProvider(\n",
    "    sources={\"connection\": dirs[\"output_db_dir\"], \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "\n",
    "# create data pipelines for ML training and evaluation\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": dirs[\"output_dataset_dir\"]\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "\n",
    "# combinator = LinearCombinator(\n",
    "#     k_sampler=uniform_k(2, 8),  # Combine 2-4 basis items\n",
    "#     coef_sampler=lambda rng: rng.uniform(0.5, 1.5),  # Random coefficients\n",
    "# )\n",
    "\n",
    "# ======== pattern decomposition combinator using SGM ========\n",
    "d = 256\n",
    "dim = (d, d)\n",
    "canvas = pattern_gen.DynamicPatterns(*dim)\n",
    "canvas._distributions = [pattern_gen.StaticGaussianDistribution(canvas) for _ in range(100)]\n",
    "stream = canvas.pattern_stream(std_1=0.03, std_2=0.2, max_intensity=100, fade_rate=0.96, distribution='other')\n",
    "combinator = PatternDecompositionCombinator(\n",
    "    pattern_provider=stream,\n",
    "    decomposition_method=\"nnls\",\n",
    ")\n",
    "\n",
    "val_provider, test_provider = eval_provider.split(config[\"data\"][\"val_test_split\"])\n",
    "train_pipeline = CachedBasisPipeline(train_provider, combinator=combinator, transforms=transforms, eager=True)\n",
    "val_pipeline = PyTorchPipeline(val_provider, transforms[:-1]).to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "test_pipeline = PyTorchPipeline(test_provider, transforms[:-1]).to_memory_dataset(config[\"data\"][\"dataset_ops\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dataset (train, val, test) takes some random samples for visualization and sanity check \n",
    "# Specifically for the training set, also print out the basis combination info\n",
    "for sample in train_pipeline:\n",
    "    record = combinator.last_record\n",
    "    print(f\"Indices: {record.indices}\")\n",
    "    print(f\"Coefficients: {record.coefficients}\")\n",
    "    # Get the actual basis components\n",
    "    for idx, coef in zip(record.indices, record.coefficients):\n",
    "        basis_item = train_pipeline.get_basis(idx)\n",
    "        print(f\"  basis[{idx}] * {coef:.2f}\")\n",
    "    left_parts, right_parts = sample\n",
    "    plot_image(left_parts[0], title=\"Input (Fiber Output)\", figsize=(6,6))\n",
    "    plot_image(right_parts[0], title=\"Ground Truth\", figsize=(6,6))\n",
    "    break\n",
    "\n",
    "for left_parts, right_parts in val_pipeline:\n",
    "    plot_image(left_parts[0], title=\"Input (Fiber Output)\", figsize=(6,6))\n",
    "    plot_image(right_parts[0], title=\"Ground Truth\", figsize=(6,6))\n",
    "    break\n",
    "for left_parts, right_parts in test_pipeline:\n",
    "    plot_image(left_parts[0], title=\"Input (Fiber Output)\", figsize=(6,6))\n",
    "    plot_image(right_parts[0], title=\"Ground Truth\", figsize=(6,6))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a947368",
   "metadata": {},
   "source": [
    "# Scope 2 - Real data only with data augmentation pipeline (super position)\n",
    "Create such ready to use dataset for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da7aac",
   "metadata": {},
   "source": [
    "# Visualization (temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xflow.utils.visualization import stack_log_remap, stack_linear_clip\n",
    "stacked = stack_log_remap([x[1] for x in results])\n",
    "plot_image(stacked)\n",
    "\n",
    "stacked = stack_linear_clip([x[1] for x in results])\n",
    "plot_image(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "Point = Tuple[float, float]\n",
    "\n",
    "\n",
    "def _min_mass_segment(weights: np.ndarray, frac: float, eps: float = 1e-12) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Smallest contiguous index range [i, j) whose sum >= frac * total.\n",
    "    Returns (i, j) with j exclusive.\n",
    "    \"\"\"\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    n = w.size\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty weights.\")\n",
    "    total = float(w.sum())\n",
    "    if total <= eps:\n",
    "        # No mass -> return full range\n",
    "        return 0, n\n",
    "\n",
    "    target = frac * total\n",
    "    best_i, best_j = 0, n\n",
    "    best_len = n + 1\n",
    "\n",
    "    j = 0\n",
    "    s = 0.0\n",
    "    for i in range(n):\n",
    "        while j < n and s < target:\n",
    "            s += w[j]\n",
    "            j += 1\n",
    "        if s >= target:\n",
    "            if (j - i) < best_len:\n",
    "                best_len = j - i\n",
    "                best_i, best_j = i, j\n",
    "        s -= w[i]\n",
    "\n",
    "    return best_i, best_j\n",
    "\n",
    "\n",
    "def _clamp_interval(a: float, b: float, lo: float = 0.0, hi: float = 1.0) -> Tuple[float, float]:\n",
    "    \"\"\"Clamp [a,b] into [lo,hi] by shifting (keeps length if possible).\"\"\"\n",
    "    length = b - a\n",
    "    if length >= (hi - lo):\n",
    "        return lo, hi\n",
    "    if a < lo:\n",
    "        b = b + (lo - a)\n",
    "        a = lo\n",
    "    if b > hi:\n",
    "        a = a - (b - hi)\n",
    "        b = hi\n",
    "    a = max(lo, a)\n",
    "    b = min(hi, b)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def square_from_projections(\n",
    "    img: np.ndarray,\n",
    "    frac: float,\n",
    "    *,\n",
    "    make_square: bool = True,\n",
    "    channel_reduce: str = \"sum\",  # \"sum\" or \"mean\"\n",
    "    eps: float = 1e-12,\n",
    ") -> Tuple[Point, Point]:\n",
    "    \"\"\"\n",
    "    1) Project image onto x and y by summing pixels.\n",
    "    2) Find minimal contiguous x-interval containing `frac` of x-projection mass,\n",
    "       and same for y.\n",
    "    3) Form rectangle. If make_square=True, expand to the smallest axis-aligned square\n",
    "       that contains that rectangle (centered), clamped to [0,1].\n",
    "\n",
    "    Returns (top_left, bottom_right) in normalized coords, with (0,0) at top-left.\n",
    "    \"\"\"\n",
    "    if frac > 1.0:\n",
    "        frac = frac / 100.0\n",
    "    if not (0.0 < frac <= 1.0):\n",
    "        raise ValueError(\"frac must be in (0,1] or (0,100].\")\n",
    "\n",
    "    a = np.asarray(img, dtype=float)\n",
    "    if a.ndim == 3:\n",
    "        if channel_reduce == \"mean\":\n",
    "            a = a.mean(axis=2)\n",
    "        elif channel_reduce == \"sum\":\n",
    "            a = a.sum(axis=2)\n",
    "        else:\n",
    "            raise ValueError(\"channel_reduce must be 'sum' or 'mean'\")\n",
    "    elif a.ndim != 2:\n",
    "        raise ValueError(\"img must be 2D or 3D array\")\n",
    "\n",
    "    # Ensure non-negative \"mass\"\n",
    "    a = np.clip(a, 0.0, None)\n",
    "\n",
    "    H, W = a.shape\n",
    "    if H == 0 or W == 0:\n",
    "        raise ValueError(\"img has zero size\")\n",
    "\n",
    "    proj_x = a.sum(axis=0)  # length W\n",
    "    proj_y = a.sum(axis=1)  # length H\n",
    "\n",
    "    ix0, ix1 = _min_mass_segment(proj_x, frac, eps=eps)  # [ix0, ix1)\n",
    "    iy0, iy1 = _min_mass_segment(proj_y, frac, eps=eps)  # [iy0, iy1)\n",
    "\n",
    "    # Convert pixel-edge indices to normalized [0,1]\n",
    "    x0, x1 = ix0 / W, ix1 / W\n",
    "    y0, y1 = iy0 / H, iy1 / H\n",
    "\n",
    "    if not make_square:\n",
    "        return (x0, y0), (x1, y1)\n",
    "\n",
    "    # Expand rectangle to a square (axis-aligned), centered on the rectangle\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    side = max(w, h)\n",
    "\n",
    "    cx = 0.5 * (x0 + x1)\n",
    "    cy = 0.5 * (y0 + y1)\n",
    "\n",
    "    sx0, sx1 = cx - 0.5 * side, cx + 0.5 * side\n",
    "    sy0, sy1 = cy - 0.5 * side, cy + 0.5 * side\n",
    "\n",
    "    sx0, sx1 = _clamp_interval(sx0, sx1, 0.0, 1.0)\n",
    "    sy0, sy1 = _clamp_interval(sy0, sy1, 0.0, 1.0)\n",
    "\n",
    "    return (sx0, sy0), (sx1, sy1)\n",
    "\n",
    "\n",
    "# Example:\n",
    "tl, br = square_from_projections(stacked, 0.96)\n",
    "highlighted = draw_red_square(stacked, tl, br, thickness=2)\n",
    "plot_image(highlighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array(results)\n",
    "hc, vc, hw, vw = a[:, 0], a[:, 1], a[:, 2], a[:, 3]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n",
    "\n",
    "ax[0].scatter(hc, vc, s=5, alpha=0.3)\n",
    "ax[0].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_centroid\", ylabel=\"v_centroid\", title=\"Centroids\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "\n",
    "ax[1].scatter(hw, vw, s=5, alpha=0.3)\n",
    "ax[1].set(xlim=(0, 1), ylim=(0, 1), xlabel=\"h_width\", ylabel=\"v_width\", title=\"Widths\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d613e",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and read image sample paths\n",
    "# \"\"\"wednesday chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-19\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE \n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"Friday Chromox\"\"\"\n",
    "dirs = config[\"paths\"][\"chromox_2025-11-21\"] \n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM \n",
    "    mmf_dataset_metadata \n",
    "WHERE\n",
    "    batch IN (1, 2, 3)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"Saturday Chromox\"\"\"\n",
    "# dirs = config[\"paths\"][\"chromox_2025-11-22-morning\"] \n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     image_path\n",
    "# FROM \n",
    "#     mmf_dataset_metadata \n",
    "# WHERE\n",
    "#     batch IN (10, 11, 12)\n",
    "# --LIMIT 20\n",
    "# \"\"\"\n",
    "\n",
    "db_path = f\"{dirs}/db/dataset_meta.db\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "image_paths = realbeam_provider()\n",
    "print(f\"Found {len(image_paths)} entries in the database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
