{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59334af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrewxu/Documents/GitHub/fiber-image-reconstruction\n",
      "[config_utils] Using machine profile: mac-andrewxu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path.cwd().parent)   # go one level up\n",
    "print(os.getcwd())            # check\n",
    "\n",
    "# pip install xflow-py\n",
    "from xflow import ConfigManager, SqlProvider, PyTorchPipeline, show_model_info\n",
    "from xflow.data import build_transforms_from_config\n",
    "from xflow.utils import load_validated_config, save_image\n",
    "import xflow.extensions.physics\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import tarfile\n",
    "from datetime import datetime  \n",
    "from config_utils import load_config\n",
    "from utils import *\n",
    "\n",
    "# Create experiment output directory  (timestamped)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")  \n",
    "\n",
    "experiment_name = \"CLEAR_make_dataset\"  # TM, SHL_DNN, U_Net, Pix2pix, ERN, CAE, SwinT, CAE_syth\n",
    "folder_name = f\"{experiment_name}-{timestamp}\"  \n",
    "config_manager = ConfigManager(load_config(f\"{experiment_name}.yaml\", \n",
    "                                           experiment_name=folder_name))\n",
    "config = config_manager.get()\n",
    "config_manager.add_files(config[\"extra_files\"])\n",
    "experiment_output_dir = config[\"paths\"][\"output\"]\n",
    "        \n",
    "def make_dataset(provider, transforms):\n",
    "    pipeline = PyTorchPipeline(provider, transforms)\n",
    "    dataset = pipeline.to_memory_dataset(config[\"data\"][\"dataset_ops\"])\n",
    "    return dataset, pipeline.in_memory_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3aa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (Wednesday Chromox)\n",
    "# ====================\n",
    "\n",
    "test_dir = config[\"paths\"][\"chromox_01\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (10, 11, 12)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "train_provider, evaluation_provider = realbeam_provider.split(ratio=config[\"data\"][\"train_val_split\"], seed=config[\"seed\"])\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": test_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_dataset, n1 = make_dataset(train_provider, transforms)\n",
    "val_dataset, n2 = make_dataset(val_provider, transforms)\n",
    "test_dataset, n3 = make_dataset(test_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (Friday + Saturday Chromox)\n",
    "# ====================\n",
    "test_dir = config[\"paths\"][\"chromox_02\"]\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    '{test_dir}' || '/' || image_path AS image_path\n",
    "FROM mmf_dataset_metadata\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "\n",
    "test_dir = config[\"paths\"][\"chromox_03\"]\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    '{test_dir}' || '/' || image_path AS image_path\n",
    "FROM mmf_dataset_metadata\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = realbeam_provider.merge(SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    "))\n",
    "\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "realbeam_provider, n1 = make_dataset(realbeam_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (WWednesday DMD)\n",
    "# ====================\n",
    "test_dir = config[\"paths\"][\"dmd_01\"]\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    '{test_dir}' || '/' || image_path AS image_path\n",
    "FROM mmf_dataset_metadata\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "realbeam_provider, n1 = make_dataset(realbeam_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2510f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== \n",
    "# Prepare Dataset (YAG screen)\n",
    "# ====================\n",
    "\n",
    "test_dir = config[\"paths\"][\"test_set\"]\n",
    "# Create SqlProvider to query the database\n",
    "db_path = f\"{test_dir}/db/dataset_meta.db\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    image_path\n",
    "FROM mmf_dataset_metadata \n",
    "WHERE batch IN (1, 7)\n",
    "--LIMIT 20\n",
    "\"\"\"\n",
    "realbeam_provider = SqlProvider(\n",
    "    sources={\"connection\": db_path, \"sql\": query}, output_config={'list': \"image_path\"}\n",
    ")\n",
    "train_provider, evaluation_provider = realbeam_provider.split(ratio=config[\"data\"][\"train_val_split\"], seed=config[\"seed\"])\n",
    "val_provider, test_provider = evaluation_provider.split(ratio=config[\"data\"][\"val_test_split\"], seed=config[\"seed\"])\n",
    "\n",
    "# For train dataset\n",
    "config[\"data\"][\"transforms\"][\"torch\"].insert(0, {\n",
    "    \"name\": \"add_parent_dir\",\n",
    "    \"params\": {\n",
    "        \"parent_dir\": test_dir\n",
    "    }\n",
    "})\n",
    "transforms = build_transforms_from_config(config[\"data\"][\"transforms\"][\"torch\"])\n",
    "train_dataset, n1 = make_dataset(train_provider, transforms)\n",
    "val_dataset, n2 = make_dataset(val_provider, transforms)\n",
    "test_dataset, n3 = make_dataset(test_provider, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f870993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples in providers: \",len(train_provider),len(val_provider),len(test_provider))\n",
    "print(\"Total samples in datasets:\", n1, n2, n3)\n",
    "print(\"Batch: \",len(train_dataset),len(val_dataset),len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
